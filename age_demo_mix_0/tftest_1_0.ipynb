{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ABIDE I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from progressbar import *\n",
    "import nibabel as nib\n",
    "import pdb\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from my_tools import *\n",
    "import tarfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from my_tools import *\n",
    "import demo_1_1\n",
    "import try_demo_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mkdir('./img')\n",
    "my_mkdir('./log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo_1_1.py --num_epochs=10 --batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python demo_1_2.py --num_epochs=200 --MIN_TEST_LOSS=1000.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_demo_1_1.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.2962962962963"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2500/54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-31 11:22:39.581181: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-03-31 11:22:39.676816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-03-31 11:22:39.677214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 10.54GiB\n",
      "2019-03-31 11:22:39.677228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2019-03-31 11:22:39.852712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-31 11:22:39.852741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2019-03-31 11:22:39.852749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2019-03-31 11:22:39.852909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10189 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Let's get started to train!\n",
      "Step 0: training_loss = 56115608.00 (3.591 sec)\n",
      " (val_loss)test_loss = 90600664.00\n",
      "Step 100: training_loss = 608.77 (0.859 sec)\n",
      " (val_loss)test_loss = 290.99\n",
      "best shot model: test_loss = 290.99, step = 100\n",
      "Step 200: training_loss = 445.23 (0.863 sec)\n",
      " (val_loss)test_loss = 273.22\n",
      "best shot model: test_loss = 273.22, step = 200\n",
      "model saved successfully.\n",
      "Step 300: training_loss = 182.00 (0.874 sec)\n",
      " (val_loss)test_loss = 241.69\n",
      "best shot model: test_loss = 241.69, step = 300\n",
      "model saved successfully.\n",
      "Step 400: training_loss = 292.71 (0.868 sec)\n",
      " (val_loss)test_loss = 224.04\n",
      "best shot model: test_loss = 224.04, step = 400\n",
      "model saved successfully.\n",
      "Step 500: training_loss = 173.93 (0.862 sec)\n",
      " (val_loss)test_loss = 208.38\n",
      "best shot model: test_loss = 208.38, step = 500\n",
      "model saved successfully.\n",
      "Step 600: training_loss = 89.30 (0.871 sec)\n",
      " (val_loss)test_loss = 185.59\n",
      "best shot model: test_loss = 185.59, step = 600\n",
      "model saved successfully.\n",
      "Step 700: training_loss = 105.34 (0.871 sec)\n",
      " (val_loss)test_loss = 175.54\n",
      "best shot model: test_loss = 175.54, step = 700\n",
      "model saved successfully.\n",
      "Step 800: training_loss = 202.44 (0.868 sec)\n",
      " (val_loss)test_loss = 164.60\n",
      "best shot model: test_loss = 164.60, step = 800\n",
      "model saved successfully.\n",
      "Step 900: training_loss = 148.23 (0.883 sec)\n",
      " (val_loss)test_loss = 153.99\n",
      "best shot model: test_loss = 153.99, step = 900\n",
      "model saved successfully.\n",
      "Step 1000: training_loss = 146.64 (0.865 sec)\n",
      " (val_loss)test_loss = 143.50\n",
      "best shot model: test_loss = 143.50, step = 1000\n",
      "model saved successfully.\n",
      "Step 1100: training_loss = 113.75 (0.909 sec)\n",
      " (val_loss)test_loss = 130.19\n",
      "best shot model: test_loss = 130.19, step = 1100\n",
      "model saved successfully.\n",
      "Step 1200: training_loss = 60.96 (0.879 sec)\n",
      " (val_loss)test_loss = 123.17\n",
      "best shot model: test_loss = 123.17, step = 1200\n",
      "model saved successfully.\n",
      "Step 1300: training_loss = 44.55 (0.877 sec)\n",
      " (val_loss)test_loss = 113.50\n",
      "best shot model: test_loss = 113.50, step = 1300\n",
      "model saved successfully.\n",
      "Step 1400: training_loss = 188.05 (0.891 sec)\n",
      " (val_loss)test_loss = 105.60\n",
      "best shot model: test_loss = 105.60, step = 1400\n",
      "model saved successfully.\n",
      "Step 1500: training_loss = 310.89 (0.873 sec)\n",
      " (val_loss)test_loss = 96.62\n",
      "best shot model: test_loss = 96.62, step = 1500\n",
      "model saved successfully.\n",
      "Step 1600: training_loss = 99.66 (0.885 sec)\n",
      " (val_loss)test_loss = 88.60\n",
      "best shot model: test_loss = 88.60, step = 1600\n",
      "model saved successfully.\n",
      "Step 1700: training_loss = 70.81 (0.868 sec)\n",
      " (val_loss)test_loss = 82.59\n",
      "best shot model: test_loss = 82.59, step = 1700\n",
      "model saved successfully.\n",
      "Step 1800: training_loss = 91.79 (0.864 sec)\n",
      " (val_loss)test_loss = 77.40\n",
      "best shot model: test_loss = 77.40, step = 1800\n",
      "model saved successfully.\n",
      "Step 1900: training_loss = 70.77 (0.876 sec)\n",
      " (val_loss)test_loss = 71.71\n",
      "best shot model: test_loss = 71.71, step = 1900\n",
      "model saved successfully.\n",
      "Step 2000: training_loss = 75.77 (0.865 sec)\n",
      " (val_loss)test_loss = 65.31\n",
      "best shot model: test_loss = 65.31, step = 2000\n",
      "model saved successfully.\n",
      "Step 2100: training_loss = 110.10 (0.869 sec)\n",
      " (val_loss)test_loss = 61.36\n",
      "best shot model: test_loss = 61.36, step = 2100\n",
      "model saved successfully.\n",
      "Step 2200: training_loss = 11.61 (0.868 sec)\n",
      " (val_loss)test_loss = 61.54\n",
      "Step 2300: training_loss = 48.16 (0.871 sec)\n",
      " (val_loss)test_loss = 59.77\n",
      "best shot model: test_loss = 59.77, step = 2300\n",
      "model saved successfully.\n",
      "Done training for 46 epochs, 2365 steps.\n",
      "l1_conv3d/Relu   [None, 130, 130, 130, 16]\n",
      "l1_conv3d/max_pool3d   [None, 65, 65, 65, 16]\n",
      "l2_conv3d/Relu   [None, 65, 65, 65, 32]\n",
      "l2_conv3d/max_pool3d   [None, 33, 33, 33, 32]\n",
      "l3_conv3d/Relu   [None, 33, 33, 33, 64]\n",
      "l3_conv3d/max_pool3d   [None, 17, 17, 17, 64]\n",
      "l4_conv3d/Relu   [None, 17, 17, 17, 64]\n",
      "l4_conv3d/max_pool3d   [None, 9, 9, 9, 64]\n",
      "l5_conv3d/Relu   [None, 9, 9, 9, 64]\n",
      "l5_conv3d/max_pool3d   [None, 5, 5, 5, 64]\n",
      "l6_fc/fc_out1   [None, 512]\n",
      "l6_fc/dropout1/mul   [None, 512]\n",
      "l7_fc/fc_out2   [None, 128]\n",
      "l7_fc/dropout2/mul   [None, 128]\n",
      "l8_fc/final_output   [None, 1]\n",
      "2019-03-31 11:57:49.889624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2019-03-31 11:57:49.889652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-31 11:57:49.889657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2019-03-31 11:57:49.889661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2019-03-31 11:57:49.889741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10189 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Model loaded successfully.\n",
      "test_loss = 59.77, test_accuracy = 0.00.\n",
      "Figure(1500x500)\n",
      "Figure(640x480)\n",
      "MaxPool3D   [None, 65, 65, 65, 8]\n",
      "MaxPool3D_1   [None, 33, 33, 33, 16]\n",
      "MaxPool3D_2   [None, 17, 17, 17, 32]\n",
      "MaxPool3D_3   [None, 9, 9, 9, 64]\n",
      "MaxPool3D_4   [None, 5, 5, 5, 128]\n",
      "dropout_5/mul   [None, 1024]\n",
      "output   [None, 1]\n",
      "2019-03-31 11:57:55.900419: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-03-31 11:57:55.974261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-03-31 11:57:55.974624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 10.46GiB\n",
      "2019-03-31 11:57:55.974640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2019-03-31 11:57:56.138533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-31 11:57:56.138560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2019-03-31 11:57:56.138568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2019-03-31 11:57:56.138719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Let's get started to train!\n",
      "Step 0: training_loss = 136233.31 (2.455 sec)\n",
      " (val_loss)test_loss = 203009.66\n",
      "Step 100: training_loss = 35283.25 (0.551 sec)\n",
      " (val_loss)test_loss = 9517.58\n",
      "Step 200: training_loss = 1042.87 (0.537 sec)\n",
      " (val_loss)test_loss = 12023.05\n",
      "Step 300: training_loss = 2018.98 (0.536 sec)\n",
      " (val_loss)test_loss = 3932.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: training_loss = 11436.74 (0.535 sec)\n",
      " (val_loss)test_loss = 2957.82\n",
      "Step 500: training_loss = 342.21 (0.543 sec)\n",
      " (val_loss)test_loss = 2028.06\n",
      "Step 600: training_loss = 4205.22 (0.528 sec)\n",
      " (val_loss)test_loss = 297.07\n",
      "best shot model: test_loss = 297.07, step = 600\n",
      "model saved successfully.\n",
      "Step 700: training_loss = 524.73 (0.541 sec)\n",
      " (val_loss)test_loss = 2156.76\n",
      "Step 800: training_loss = 559.86 (0.540 sec)\n",
      " (val_loss)test_loss = 249.67\n",
      "best shot model: test_loss = 249.67, step = 800\n",
      "model saved successfully.\n",
      "Step 900: training_loss = 396.74 (0.537 sec)\n",
      " (val_loss)test_loss = 1139.19\n",
      "Step 1000: training_loss = 725.35 (0.537 sec)\n",
      " (val_loss)test_loss = 2266.85\n",
      "Step 1100: training_loss = 688.18 (0.537 sec)\n",
      " (val_loss)test_loss = 5480.15\n",
      "Step 1200: training_loss = 189.84 (0.542 sec)\n",
      " (val_loss)test_loss = 348.56\n",
      "Step 1300: training_loss = 126.34 (0.538 sec)\n",
      " (val_loss)test_loss = 326.01\n",
      "Step 1400: training_loss = 1037.46 (0.560 sec)\n",
      " (val_loss)test_loss = 3850.60\n",
      "Step 1500: training_loss = 550.28 (0.536 sec)\n",
      " (val_loss)test_loss = 327.28\n",
      "Step 1600: training_loss = 170.06 (0.534 sec)\n",
      " (val_loss)test_loss = 428.85\n",
      "Step 1700: training_loss = 329.85 (0.538 sec)\n",
      " (val_loss)test_loss = 381.74\n",
      "Step 1800: training_loss = 895.46 (0.544 sec)\n",
      " (val_loss)test_loss = 394.43\n",
      "Step 1900: training_loss = 1723.76 (0.539 sec)\n",
      " (val_loss)test_loss = 287.10\n",
      "Step 2000: training_loss = 950.41 (0.531 sec)\n",
      " (val_loss)test_loss = 159.66\n",
      "best shot model: test_loss = 159.66, step = 2000\n",
      "model saved successfully.\n",
      "Step 2100: training_loss = 7454.31 (0.537 sec)\n",
      " (val_loss)test_loss = 111.96\n",
      "best shot model: test_loss = 111.96, step = 2100\n",
      "model saved successfully.\n",
      "Step 2200: training_loss = 436.57 (0.537 sec)\n",
      " (val_loss)test_loss = 944.27\n",
      "Step 2300: training_loss = 206.62 (0.544 sec)\n",
      " (val_loss)test_loss = 371.37\n",
      "Step 2400: training_loss = 87.27 (0.531 sec)\n",
      " (val_loss)test_loss = 220.33\n",
      "Step 2500: training_loss = 115.72 (0.536 sec)\n",
      " (val_loss)test_loss = 136.72\n",
      "Done training for 50 epochs, 2570 steps.\n",
      "MaxPool3D   [None, 65, 65, 65, 8]\n",
      "MaxPool3D_1   [None, 33, 33, 33, 16]\n",
      "MaxPool3D_2   [None, 17, 17, 17, 32]\n",
      "MaxPool3D_3   [None, 9, 9, 9, 64]\n",
      "MaxPool3D_4   [None, 5, 5, 5, 128]\n",
      "dropout_5/mul   [None, 1024]\n",
      "output   [None, 1]\n",
      "2019-03-31 12:21:37.756642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2019-03-31 12:21:37.756674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-31 12:21:37.756681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2019-03-31 12:21:37.756687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2019-03-31 12:21:37.756771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10118 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Model loaded successfully.\n",
      "test_loss(RMSE) = 111.96.\n",
      "Figure(1500x500)\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python demo_1_1.py --num_epochs=46 \n",
    "!python demo_1_2.py --num_epochs=50 --MIN_TEST_LOSS=1000.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_conv3d/Relu   [None, 130, 130, 130, 16]\n",
      "l1_conv3d/max_pool3d   [None, 65, 65, 65, 16]\n",
      "l2_conv3d/Relu   [None, 65, 65, 65, 32]\n",
      "l2_conv3d/max_pool3d   [None, 33, 33, 33, 32]\n",
      "l3_conv3d/Relu   [None, 33, 33, 33, 64]\n",
      "l3_conv3d/max_pool3d   [None, 17, 17, 17, 64]\n",
      "l4_conv3d/Relu   [None, 17, 17, 17, 64]\n",
      "l4_conv3d/max_pool3d   [None, 9, 9, 9, 64]\n",
      "l5_conv3d/Relu   [None, 9, 9, 9, 64]\n",
      "l5_conv3d/max_pool3d   [None, 5, 5, 5, 64]\n",
      "l6_fc/fc_out1   [None, 512]\n",
      "l6_fc/dropout1/mul   [None, 512]\n",
      "l7_fc/fc_out2   [None, 128]\n",
      "l7_fc/dropout2/mul   [None, 128]\n",
      "l8_fc/final_output   [None, 1]\n",
      "2019-03-31 11:20:15.506859: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-03-31 11:20:15.595197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-03-31 11:20:15.595561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 10.54GiB\n",
      "2019-03-31 11:20:15.595575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2019-03-31 11:20:15.774666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-31 11:20:15.774694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2019-03-31 11:20:15.774699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2019-03-31 11:20:15.774854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10189 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Model loaded successfully.\n",
      "test_loss = 44.55, test_accuracy = 0.00.\n",
      "Figure(640x480)\n",
      "MaxPool3D   [None, 65, 65, 65, 8]\n",
      "MaxPool3D_1   [None, 33, 33, 33, 16]\n",
      "MaxPool3D_2   [None, 17, 17, 17, 32]\n",
      "MaxPool3D_3   [None, 9, 9, 9, 64]\n",
      "MaxPool3D_4   [None, 5, 5, 5, 128]\n",
      "dropout_5/mul   [None, 1024]\n",
      "output   [None, 1]\n",
      "2019-03-31 11:20:23.122161: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-03-31 11:20:23.194648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-03-31 11:20:23.195011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 10.91GiB freeMemory: 10.54GiB\n",
      "2019-03-31 11:20:23.195024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2019-03-31 11:20:23.364625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-03-31 11:20:23.364649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2019-03-31 11:20:23.364654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2019-03-31 11:20:23.364823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10189 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Model loaded successfully.\n",
      "test_loss(RMSE) = 203.01.\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python demo_1_1.py --num_epochs=200 --for_test=True\n",
    "!python demo_1_2.py --num_epochs=100 --MIN_TEST_LOSS=1000.0 --for_test=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_equal(return_list):\n",
    "    decode_arr = return_list[0]\n",
    "\n",
    "    decode_label = return_list[1]\n",
    "    decode_label = np.round(decode_label,2)\n",
    "    decode_id = return_list[2]\n",
    "    decode_X = return_list[3]\n",
    "\n",
    "    #             print_sep()\n",
    "    for j in range(BATCH_SIZE):\n",
    "        d_a = decode_arr[j]\n",
    "        d_l = decode_label[j]\n",
    "        d_i = decode_id[j]\n",
    "\n",
    "        d_x = decode_X[j]\n",
    "        d_x = np.reshape(d_x,SHAPE)\n",
    "\n",
    "        npy_filename = str(d_i) + '.npy'\n",
    "        npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "    #                 print(npy_filename)\n",
    "\n",
    "        try:\n",
    "            arr_npy = np.load(npy_path_filename)\n",
    "        except FileNotFoundError:\n",
    "            print('No such file: ',npy_filename)\n",
    "            continue\n",
    "        label = round(info_df['age'][info_df['id']==d_i].values[0],2)\n",
    "        arr = arr_npy.astype(np.float32)\n",
    "\n",
    "        if np.sum(d_x!=arr) != 0:\n",
    "            print('d_x != arr')\n",
    "\n",
    "\n",
    "        if round(label-d_l,2)!=0 or np.sum(d_a!=arr)!=0:\n",
    "            print_sep()\n",
    "        #         print('i= ',i)\n",
    "            print(npy_path_filename)\n",
    "            print('original label: ',label)\n",
    "        #         print2d(arr)\n",
    "            print('extracted label: ',d_l)\n",
    "        #         print2d(decode_arr)\n",
    "            print('label == decode_label: ',round(label-d_l,2)==0)\n",
    "            print('arr == decode_arr: ',np.sum(d_a!=arr))\n",
    "    #         break\n",
    "\n",
    "\n",
    "        if d_i == 51075:\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "    print('try_equal finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lost:\n",
    "51310.npy\n",
    "51270.npy\n",
    "\n",
    "'''\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "            'label': tf.FixedLenFeature([],tf.float32),\n",
    "            'id': tf.FixedLenFeature([],tf.int64),\n",
    "        }\n",
    "    )\n",
    "    arr = tf.decode_raw(features['arr_raw'],tf.float32)\n",
    "    arr = tf.reshape(arr,list(SHAPE))\n",
    "#     arr = tf.cast(arr,tf.int32)\n",
    "#     label = tf.cast(features['label'],tf.int32)\n",
    "    label = features['label']\n",
    "    sub_id = features['id']\n",
    "    \n",
    "    return arr,label,sub_id\n",
    "\n",
    "\n",
    "npy_dir='./data_npy/mean_subtracted/'\n",
    "SHAPE = np.load('./data_npy/mean_npy.npy').shape\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "raw_image_dataset = tf.data.TFRecordDataset('try_training.tfrec')\n",
    "info_df = pd.read_csv('./training.csv', sep=',',header=0)\n",
    "\n",
    "raw_image_dataset = tf.data.TFRecordDataset('try_test.tfrec')\n",
    "info_df = pd.read_csv('./test.csv', sep=',',header=0)\n",
    "\n",
    "# Create a dictionary describing the features.  \n",
    "image_feature_description = {\n",
    "    'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "    'label': tf.FixedLenFeature([],tf.float32),\n",
    "    'id': tf.FixedLenFeature([],tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "# parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(decode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parsed_image_dataset = parsed_image_dataset.repeat(10)\n",
    "dataset = parsed_image_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "train_iterator = dataset.make_one_shot_iterator()\n",
    "# print(train_iterator.output_types)\n",
    "\n",
    "# with tf.Graph().as_default():\n",
    "handle = tf.placeholder(tf.string,shape=[])\n",
    "# iterator = tf.data.Iterator.from_string_handle(handle, train_iterator.output_types,train_iterator.output_shapes)\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, train_iterator.output_types)\n",
    "return_batch = iterator.get_next()\n",
    "\n",
    "\n",
    "arr_batch= return_batch[0]\n",
    "label_batch = return_batch[1]\n",
    "id_batch = return_batch[2]\n",
    "\n",
    "# print(type(return_batch))\n",
    "# print(return_batch)\n",
    "\n",
    "X = tf.reshape(arr_batch, [-1]+list(SHAPE)+[1])\n",
    "d_X = tf.reshape(X,[-1]+list(SHAPE))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_iterator_handle = sess.run(train_iterator.string_handle())\n",
    "    try:\n",
    "        while True:\n",
    "            return_list = sess.run([return_batch, X, d_X], feed_dict={handle:train_iterator_handle})\n",
    "            decode_arr = return_list[0][0]\n",
    "\n",
    "            decode_label = return_list[0][1]\n",
    "            decode_label = np.round(decode_label,2)\n",
    "            decode_id = return_list[0][2]\n",
    "            decode_X = return_list[1]\n",
    "            decode_d_X = return_list[2]\n",
    "            \n",
    "#             print_sep()\n",
    "            for j in range(BATCH_SIZE):\n",
    "                d_a = decode_arr[j]\n",
    "                d_l = decode_label[j]\n",
    "                d_i = decode_id[j]\n",
    "                \n",
    "                d_x = decode_X[j]\n",
    "                d_x = np.reshape(d_x,SHAPE)\n",
    "#                 print(d_x.shape)\n",
    "                d_dx = decode_d_X[j]\n",
    "#                 print(d_dx.shape)\n",
    "                \n",
    "#                 break\n",
    "                npy_filename = str(d_i) + '.npy'\n",
    "                npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "#                 print(npy_filename)\n",
    "\n",
    "                try:\n",
    "                    arr_npy = np.load(npy_path_filename)\n",
    "                except FileNotFoundError:\n",
    "                    print('No such file: ',npy_filename)\n",
    "                    continue\n",
    "                label = round(info_df['age'][info_df['id']==d_i].values[0],2)\n",
    "                arr = arr_npy.astype(np.float32)\n",
    "                \n",
    "                if np.sum(d_x!=d_dx) != 0:\n",
    "                    print('d_x != d_dx')\n",
    "                if np.sum(d_x!=arr) != 0:\n",
    "                    print('d_x != arr')\n",
    "                \n",
    "                \n",
    "                if round(label-d_l,2)!=0 or np.sum(d_a!=arr)!=0:\n",
    "                    print_sep()\n",
    "                #         print('i= ',i)\n",
    "                    print(npy_path_filename)\n",
    "                    print('original label: ',label)\n",
    "                #         print2d(arr)\n",
    "                    print('extracted label: ',d_l)\n",
    "                #         print2d(decode_arr)\n",
    "                    print('label == decode_label: ',round(label-d_l,2)==0)\n",
    "                    print('arr == decode_arr: ',np.sum(d_a!=arr))\n",
    "        #         break\n",
    "\n",
    "\n",
    "                if d_i == 51075:\n",
    "                    print_sep()\n",
    "                    print_sep()\n",
    "                    print_sep()\n",
    "\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('end...')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from progressbar import *\n",
    "import nibabel as nib\n",
    "import pdb\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from my_tools import *\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "            'label': tf.FixedLenFeature([],tf.float32),\n",
    "            'id': tf.FixedLenFeature([],tf.int64),\n",
    "        }\n",
    "    )\n",
    "    arr = tf.decode_raw(features['arr_raw'],tf.float32)\n",
    "    arr = tf.reshape(arr,list(SHAPE))\n",
    "#     arr = tf.cast(arr,tf.int32)\n",
    "#     label = tf.cast(features['label'],tf.int32)\n",
    "    label = features['label']\n",
    "    sub_id = features['id']\n",
    "    \n",
    "    return arr,label,sub_id\n",
    "\n",
    "\n",
    "npy_dir='./data_npy/mean_subtracted/'\n",
    "SHAPE = np.load('./data_npy/mean_npy.npy').shape\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "raw_image_dataset = tf.data.TFRecordDataset('try_training.tfrec')\n",
    "info_df = pd.read_csv('./training.csv', sep=',',header=0)\n",
    "\n",
    "# Create a dictionary describing the features.  \n",
    "image_feature_description = {\n",
    "    'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "    'label': tf.FixedLenFeature([],tf.float32),\n",
    "    'id': tf.FixedLenFeature([],tf.int64),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "# parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(decode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parsed_image_dataset = parsed_image_dataset.repeat(10)\n",
    "dataset = parsed_image_dataset.batch(BATCH_SIZE)\n",
    "# print(dataset)\n",
    "# type(parsed_image_dataset)\n",
    "\n",
    "for i,image_features in enumerate(dataset):\n",
    "    \n",
    "#     print(type(image_features))\n",
    "#     print(len(image_features))\n",
    "#     print(type(image_features[0]))\n",
    "#     print(image_features)\n",
    "    decode_arr = image_features[0].numpy()\n",
    "#     print(type(decode_arr))\n",
    "#     print(decode_arr.shape)\n",
    "    \n",
    "    decode_label = image_features[1]\n",
    "#     print(decode_label.shape)\n",
    "    decode_label = np.round(decode_label,2)\n",
    "    decode_id = image_features[2]\n",
    "    decode_id = decode_id.numpy()\n",
    "#     print(decode_id.shape)\n",
    "#     print(decode_id)\n",
    "    print_sep()\n",
    "#     break\n",
    "#     decode_id = int(decode_id)\n",
    "    for j in range(BATCH_SIZE):\n",
    "        d_a = decode_arr[j]\n",
    "        d_l = decode_label[j]\n",
    "        d_i = decode_id[j]\n",
    "        \n",
    "        npy_filename = str(d_i) + '.npy'\n",
    "        npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "        print(npy_filename)\n",
    "\n",
    "        try:\n",
    "            arr_npy = np.load(npy_path_filename)\n",
    "        except FileNotFoundError:\n",
    "            print('No such file: ',npy_filename)\n",
    "            continue\n",
    "        label = round(info_df['age'][info_df['id']==d_i].values[0],2)\n",
    "        arr = arr_npy.astype(np.float32)\n",
    "\n",
    "        if round(label-d_l,2)!=0 or np.sum(d_a!=arr)!=0:\n",
    "            print_sep()\n",
    "        #         print('i= ',i)\n",
    "            print(npy_path_filename)\n",
    "            print('original label: ',label)\n",
    "        #         print2d(arr)\n",
    "            print('extracted label: ',d_l)\n",
    "        #         print2d(decode_arr)\n",
    "            print('label == decode_label: ',round(label-d_l,2)==0)\n",
    "            print('arr == decode_arr: ',np.sum(d_a!=arr))\n",
    "#         break\n",
    "        \n",
    "        \n",
    "        if d_i == 51075:\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,image_features in enumerate(dataset):\n",
    "\n",
    "    decode_arr = tf.decode_raw(image_features['arr_raw'],tf.float32)\n",
    "    print(type(decode_arr))\n",
    "    print(decode_arr.shape)\n",
    "    decode_arr = tf.reshape(decode_arr,(-1,130,130,130))\n",
    "    X = tf.reshape(decode_arr, [-1]+list(SHAPE)+[1])\n",
    "    d_X = tf.reshape(X,[-1]+list(SHAPE))\n",
    "    decode_arr = decode_arr.numpy()\n",
    "    print(type(decode_arr))\n",
    "    print(decode_arr.shape)\n",
    "#     X = X.numpy()\n",
    "#     print(X.shape)\n",
    "    d_X=d_X.numpy()\n",
    "    print(d_X.shape)\n",
    "    \n",
    "    print(np.sum(d_X!=decode_arr))\n",
    "    \n",
    "    decode_label = image_features['label']\n",
    "    print(decode_label.shape)\n",
    "    decode_label = np.round(decode_label,2)\n",
    "    decode_id = image_features['id']\n",
    "    decode_id = decode_id.numpy()\n",
    "    print(decode_id.shape)\n",
    "    print(decode_id)\n",
    "    print_sep()\n",
    "#     decode_id = int(decode_id)\n",
    "    for j in range(BATCH_SIZE):\n",
    "        d_a = decode_arr[j]\n",
    "        d_l = decode_label[j]\n",
    "        d_i = decode_id[j]\n",
    "        X_j = X[j]\n",
    "        d_x = tf.reshape(X_j,SHAPE)\n",
    "        d_x = d_x.numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        npy_filename = str(d_i) + '.npy'\n",
    "        npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "\n",
    "        try:\n",
    "            arr_npy = np.load(npy_path_filename)\n",
    "        except FileNotFoundError:\n",
    "            print('No such file: ',npy_filename)\n",
    "            continue\n",
    "        label = round(info_df['age'][info_df['id']==d_i].values[0],2)\n",
    "        arr = arr_npy.astype(np.float32)\n",
    "\n",
    "        if round(label-d_l,2)!=0 or np.sum(d_a!=arr)!=0:\n",
    "            print_sep()\n",
    "        #         print('i= ',i)\n",
    "            print(npy_path_filename)\n",
    "            print('original label: ',label)\n",
    "        #         print2d(arr)\n",
    "            print('extracted label: ',d_l)\n",
    "        #         print2d(decode_arr)\n",
    "            print('label == decode_label: ',round(label-d_l,2)==0)\n",
    "            print('arr == decode_arr: ',np.sum(d_a!=arr))\n",
    "        #         break\n",
    "        \n",
    "        if np.sum(d_x!=arr)!=0:\n",
    "            print2d(d_x)\n",
    "            print2d(arr)\n",
    "        \n",
    "        \n",
    "        if d_i == 51075:\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "            print_sep()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,image_features in enumerate(parsed_image_dataset):\n",
    "#     print(type(image_features))\n",
    "    break\n",
    "    decode_arr = tf.decode_raw(image_features['arr_raw'],tf.float32)\n",
    "    print(type(decode_arr))\n",
    "    print(decode_arr.shape)\n",
    "    break\n",
    "    \n",
    "    decode_arr = tf.reshape(decode_arr,SHAPE)\n",
    "    decode_arr = decode_arr.numpy()\n",
    "#     print(type(decode_arr))\n",
    "#     print(decode_arr.shape)\n",
    "    decode_label = image_features['label']\n",
    "    decode_label = np.round(decode_label,2)\n",
    "#     print(type(decode_label))\n",
    "#     print(decode_label)\n",
    "    decode_id = image_features['id']\n",
    "    decode_id = int(decode_id)\n",
    "#     print(type(decode_id))\n",
    "#     print(decode_id)\n",
    "#     break\n",
    "\n",
    "    npy_filename = str(decode_id) + '.npy'\n",
    "    npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "    \n",
    "    try:\n",
    "        arr_npy = np.load(npy_path_filename)\n",
    "    except FileNotFoundError:\n",
    "        print('No such file: ',npy_filename)\n",
    "        continue\n",
    "    label = round(info_df['age'][info_df['id']==decode_id].values[0],2)\n",
    "    arr = arr_npy.astype(np.float32)\n",
    "#     break\n",
    "    \n",
    "#     if round(label-decode_label,2)!=0 or np.sum(decode_arr!=arr)!=0:\n",
    "    if i%len(info_df)<8:\n",
    "        print_sep()\n",
    "        print('i= ',i)\n",
    "        print(npy_path_filename)\n",
    "        print('original label: ',label)\n",
    "    #         print2d(arr)\n",
    "        print('extracted label: ',decode_label)\n",
    "    #         print2d(decode_arr)\n",
    "        print('label == decode_label: ',round(label-decode_label,2)==0)\n",
    "        print('arr == decode_arr: ',np.sum(decode_arr!=arr))\n",
    "    if i%len(info_df)==8:\n",
    "        print_sep()\n",
    "        print_sep()\n",
    "#     break\n",
    "#     if i>8:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label-decode_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(decode_arr!=arr)!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "51075.0\n",
    "14.2\n",
    "50562.0\n",
    "10.83\n",
    "51078.0\n",
    "6.47\n",
    "50656.0\n",
    "28.0\n",
    "50269.0\n",
    "14.66\n",
    "50375.0\n",
    "11.6\n",
    "50198.0\n",
    "15.28\n",
    "50781.0\n",
    "9.3\n",
    "51360.0\n",
    "7.0\n",
    "51036.0\n",
    "8.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "under eager mode\n",
    "to check the .tfrecords has the right data\n",
    "\n",
    "'''\n",
    "\n",
    "npy_dir='./data_npy/mean_subtracted/'\n",
    "info_df = pd.read_csv('./training.csv', sep=',',header=0)\n",
    "id_list = list(info_df['id'])\n",
    "age_list = list(info_df['age'])\n",
    "n = len(id_list)\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(path='training.tfrecords')\n",
    "\n",
    "# pbar = ProgressBar().start()\n",
    "for i,string_record in enumerate(record_iterator):\n",
    "    npy_filename = str(int(id_list[i])) + '.npy'\n",
    "    npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "    \n",
    "    try:\n",
    "        arr_npy = np.load(npy_path_filename)\n",
    "    except FileNotFoundError:\n",
    "        print('No such file: ',npy_filename)\n",
    "        continue\n",
    "    label = round(age_list[i],2)\n",
    "    arr = arr_npy.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "\n",
    "    decode_label = example.features.feature['label'].float_list.value\n",
    "    decode_label = np.round(decode_label,2)\n",
    "    decode_arr = example.features.feature['arr_raw'].bytes_list.value\n",
    "    decode_arr = tf.decode_raw(decode_arr,tf.float32)\n",
    "    decode_arr = tf.reshape(decode_arr,arr.shape)\n",
    "    decode_arr = decode_arr.numpy()\n",
    "#     break\n",
    "    \n",
    "    \n",
    "#     print(label==decode_label)\n",
    "#     print(np.sum(decode_arr!=arr))\n",
    "#     print2d(decode_arr.numpy())\n",
    "    # Exit after 1 iteration as this is purely demonstrative.\n",
    "    \n",
    "    \n",
    "    if label != decode_label or np.sum(decode_arr!=arr)!=0:\n",
    "        print_sep()\n",
    "        print(i)\n",
    "        print(npy_path_filename)\n",
    "        print_sep()\n",
    "        print('original label: ',label)\n",
    "#         print2d(arr)\n",
    "        print_sep()\n",
    "        print('extracted label: ',decode_label)\n",
    "#         print2d(decode_arr)\n",
    "        print('label == decode_label: ',label==decode_label)\n",
    "        print('arr == decode_arr: ',np.sum(decode_arr!=arr))\n",
    "    \n",
    "#     pbar.update(int(i*100/(n-1)))\n",
    "# pbar.finish()\n",
    "#     if i>10:\n",
    "#         break\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1_tfrec(csv_path_name,npy_dir,tf_filename='tf.tfrecords'):\n",
    "    '''\n",
    "    To generate .tfrecord files according to .csv file\n",
    "    \n",
    "    csv_path_name: .csv file's path and name\n",
    "    npy_dir: where are the .npy files\n",
    "    tf_filename: name of .tfrecords file under ./\n",
    "    '''\n",
    "#     pdb.set_trace()\n",
    "    \n",
    "    tf_path_name = os.path.join('./',tf_filename)\n",
    "\n",
    "    if os.path.exists(tf_path_name):\n",
    "        print(tf_path_name, 'exists already.')\n",
    "        os.remove(tf_path_name)\n",
    "    print('Writing', tf_path_name)\n",
    "    with tf.python_io.TFRecordWriter(tf_path_name) as writer:\n",
    "        info_df = pd.read_csv(csv_path_name, sep=',',header=0)\n",
    "        id_list = list(info_df['id'])\n",
    "        age_list = list(info_df['age'])\n",
    "        n = len(id_list)\n",
    "        for i,ixi_id in enumerate(id_list):\n",
    "            npy_filename = str(int(ixi_id)) + '.npy'\n",
    "            npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "            print(npy_path_filename)\n",
    "            try:\n",
    "                arr_npy = np.load(npy_path_filename)\n",
    "            except FileNotFoundError:\n",
    "                print('No such file: ',npy_filename)\n",
    "                continue\n",
    "            label = round(age_list[i],2)\n",
    "            arr = arr_npy.astype(np.float32)\n",
    "            arr_raw = arr.tostring()\n",
    "            example = tf.train.Example(\n",
    "                features = tf.train.Features(\n",
    "                    feature = {\n",
    "                        'arr_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[arr_raw])),\n",
    "                        'label': tf.train.Feature(float_list=tf.train.FloatList(value=[label]))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "            break\n",
    "    return arr_npy,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr,label = test1_tfrec('./training.csv',npy_dir='./data_npy/mean_subtracted/',tf_filename='test1.tfrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)\n",
    "print2d(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode\n",
    "record_iterator = tf.python_io.tf_record_iterator(path='test1.tfrec')\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "\n",
    "#     print(example)\n",
    "    print(example.features.feature['label'].float_list.value)\n",
    "    decode_arr = example.features.feature['arr_raw'].bytes_list.value\n",
    "    decode_arr = tf.decode_raw(decode_arr,tf.float32)\n",
    "    decode_arr = tf.reshape(decode_arr,arr.shape)\n",
    "    print(decode_arr.shape)\n",
    "    print(type(decode_arr.numpy()))\n",
    "    print2d(decode_arr.numpy())\n",
    "    # Exit after 1 iteration as this is purely demonstrative.\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_iterator = tf.python_io.tf_record_iterator(path='test1.tfrec')\n",
    "for i, string_record in enumerate(record_iterator):\n",
    "    print(i)\n",
    "    print(type(string_record))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = a\n",
    "print(np.sum(a!=b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(1.4444,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_dataset = tf.data.TFRecordDataset('test1.tfrec')\n",
    "\n",
    "# Create a dictionary describing the features.  \n",
    "image_feature_description = {\n",
    "    'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "    'label': tf.FixedLenFeature([],tf.float32),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "# parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_features in parsed_image_dataset:\n",
    "    image_raw = image_features['arr_raw']\n",
    "    print(type(image_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_in_snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_in_snow  = tf.keras.utils.get_file('320px-Felis_catus-cat_on_snow.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\n",
    "williamsburg_bridge = tf.keras.utils.get_file('194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg')\n",
    "\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "image_labels = {\n",
    "    cat_in_snow : 0,\n",
    "    williamsburg_bridge : 1,\n",
    "    '/home/woody/.keras/datasets/temp0.jpg':2,\n",
    "    '/home/woody/.keras/datasets/temp1.jpg':3,\n",
    "    '/home/woody/.keras/datasets/temp2.jpg':4,\n",
    "    '/home/woody/.keras/datasets/temp6.jpg':5,\n",
    "    '/home/woody/.keras/datasets/temp8.jpg':6,\n",
    "    '/home/woody/.keras/datasets/temp9.jpg':7,\n",
    "}\n",
    "\n",
    "# # This is an example, just using the cat image.\n",
    "# image_string = open(cat_in_snow, 'rb').read()\n",
    "\n",
    "# label = image_labels[cat_in_snow]\n",
    "\n",
    "# Create a dictionary with features that may be relevant.\n",
    "def image_example(image_string, label):\n",
    "    image_shape = tf.image.decode_jpeg(image_string).shape\n",
    "\n",
    "    feature = {\n",
    "      'height': _int64_feature(image_shape[0]),\n",
    "      'width': _int64_feature(image_shape[1]),\n",
    "      'depth': _int64_feature(image_shape[2]),\n",
    "      'label': _int64_feature(label),\n",
    "      'image_raw': _bytes_feature(image_string),\n",
    "    }\n",
    "    \n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "# for line in str(image_example(image_string, label)).split('\\n')[:15]:\n",
    "#     print(line)\n",
    "# print('...')\n",
    "\n",
    "# Write the raw image files to images.tfrecords.\n",
    "# First, process the two images into tf.Example messages.\n",
    "# Then, write to a .tfrecords file.\n",
    "\n",
    "with tf.python_io.TFRecordWriter('images.tfrecords') as writer:\n",
    "    for filename, label in image_labels.items():\n",
    "        print(label)\n",
    "        image_string = open(filename, 'rb').read()\n",
    "        tf_example = image_example(image_string, label)\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n",
    "\n",
    "# Create a dictionary describing the features.  \n",
    "image_feature_description = {\n",
    "    'height': tf.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_features in parsed_image_dataset:\n",
    "    print(type(image_features['image_raw']))\n",
    "    image_raw = image_features['image_raw'].numpy()\n",
    "    print(image_features['label'])\n",
    "#     display.display(display.Image(data=image_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode\n",
    "record_iterator = tf.python_io.tf_record_iterator(path='images.tfrecords')\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    \n",
    "#     decode_arr = example.features.feature['image_raw'].bytes_list.value\n",
    "#     decode_arr = tf.decode_raw(decode_arr,tf.string)\n",
    "#     print(type(decode_arr))\n",
    "#     display.display(display.Image(data=(decode_arr)))\n",
    "    label = example.features.feature['label'].int64_list.value\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_dir='./data_npy/mean_subtracted/'\n",
    "info_df = pd.read_csv('./training.csv', sep=',',header=0)\n",
    "id_list = list(info_df['id'])\n",
    "age_list = list(info_df['age'])\n",
    "n = len(id_list)\n",
    "for i,ixi_id in enumerate(id_list):\n",
    "    npy_filename = str(int(ixi_id)) + '.npy'\n",
    "    npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "    try:\n",
    "        arr_npy = np.load(npy_path_filename)\n",
    "    except FileNotFoundError:\n",
    "        print('No such file: ',npy_filename)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_list[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "\n",
    "gen_tfrecord('./training.csv',npy_dir='./data_npy/mean_subtracted/',tf_filename='try_training.tfrec')\n",
    "gen_tfrecord('./test.csv',npy_dir='./data_npy/mean_subtracted/',tf_filename='try_test.tfrec')\n",
    "# gen_tfrecord('./test.csv',npy_dir='./data_npy/mean_subtracted/',tf_filename='test.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This could be used to check if the generated .tfrec file has the right data\n",
    "\n",
    "FOR TEST EDITION\n",
    "'''\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(path='try_training.tfrec')\n",
    "\n",
    "SHAPE = np.load('./data_npy/mean_npy.npy').shape\n",
    "\n",
    "info_df = pd.read_csv('./training.csv', sep=',',header=0)\n",
    "\n",
    "# pbar = ProgressBar().start()\n",
    "for i,string_record in enumerate(record_iterator):\n",
    "    \n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "\n",
    "    sub_id = example.features.feature['id'].int64_list.value[0]\n",
    "    \n",
    "    decode_label = example.features.feature['label'].float_list.value\n",
    "    decode_label = np.round(decode_label,2)[0]\n",
    "#     print(sub_id,decode_label)\n",
    "    decode_arr = example.features.feature['arr_raw'].bytes_list.value\n",
    "    print(type(decode_arr))\n",
    "    decode_arr = tf.decode_raw(decode_arr,tf.float32)\n",
    "    print(type(decode_arr))\n",
    "    decode_arr = tf.reshape(decode_arr,SHAPE)\n",
    "    decode_arr = decode_arr.numpy()\n",
    "    \n",
    "    break\n",
    "    \n",
    "    npy_filename = str(sub_id) + '.npy'\n",
    "    npy_path_filename = os.path.join(npy_dir,npy_filename)\n",
    "    \n",
    "    try:\n",
    "        arr_npy = np.load(npy_path_filename)\n",
    "    except FileNotFoundError:\n",
    "        print('No such file: ',npy_filename)\n",
    "        continue\n",
    "    label = round(info_df['age'][info_df['id']==sub_id].values[0],2)\n",
    "    arr = arr_npy.astype(np.float32)\n",
    "#     break\n",
    "    \n",
    "    if label != decode_label or np.sum(decode_arr!=arr)!=0:\n",
    "        print_sep()\n",
    "        print(i)\n",
    "        print(npy_path_filename)\n",
    "        print_sep()\n",
    "        print('original label: ',label)\n",
    "#         print2d(arr)\n",
    "        print_sep()\n",
    "        print('extracted label: ',decode_label)\n",
    "#         print2d(decode_arr)\n",
    "        print('label == decode_label: ',label==decode_label)\n",
    "        print('arr == decode_arr: ',np.sum(decode_arr!=arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv('./training.csv', sep=',',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = info_df['age'][info_df['id']==51095].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
