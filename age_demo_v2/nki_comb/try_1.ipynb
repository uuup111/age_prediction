{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from dev_tools.my_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as graph_raw: \n",
    "    with tf.Session(graph=graph_raw) as sess: \n",
    "        saver = tf.train.import_meta_graph('../nki_raw/log/model_mse_woody.ckpt.meta') \n",
    "        saver.restore(sess, '../nki_raw/log/model_mse_woody.ckpt')\n",
    "        writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "#         input = sess.graph.get_tensor_by_name('Placeholder:0')\n",
    "#         output = sess.graph.get_tensor_by_name('l1/out:0')\n",
    "#         print(sess.run(output,{input:for_input}))\n",
    "#         print(sess.graph_def)\n",
    "#         g1def = graph_util.convert_variables_to_constants( \n",
    "#             sess, \n",
    "#             sess.graph_def, \n",
    "# #             [\"l1/out\",'l2/out'])\n",
    "#             [\"l1/out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../../dev_models/model_woody_for_trial.py --root_dir='../nki_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Graph().as_default() as g_comb: \n",
    "    with tf.Session(graph=) as sess: \n",
    "#         with tf.name_scope('n1') as scope:\n",
    "#             saver = tf.train.import_meta_graph('../nki_raw/log/model_mse_woody.ckpt.meta') \n",
    "#             saver.restore(sess, '../nki_raw/log/model_mse_woody.ckpt')\n",
    "#         with tf.name_scope('n2') as scope:\n",
    "#             saver = tf.train.import_meta_graph('../nki_alff/log/model_mse_woody.ckpt.meta') \n",
    "#             saver.restore(sess, '../nki_alff/log/model_mse_woody.ckpt')\n",
    "        with tf.name_scope('n_1') as scope:\n",
    "            saver = tf.train.import_meta_graph('./log/model_woody_trial.ckpt.meta') \n",
    "            saver.restore(sess, './log/model_woody_trial.ckpt')\n",
    "            n1_def = graph_util.convert_variables_to_constants( \n",
    "                sess, \n",
    "                sess.graph_def, \n",
    "    #             [\"l1/out\",'l2/out'])\n",
    "                [\"l1/out\"])\n",
    "    with tf.Session(graph=g_comb) as sess: \n",
    "        with tf.name_scope('n_2') as scope:\n",
    "            saver = tf.train.import_meta_graph('./log/model_woody_trial.ckpt.meta') \n",
    "            saver.restore(sess, './log/model_woody_trial.ckpt')\n",
    "            n2_def = graph_util.convert_variables_to_constants( \n",
    "                sess, \n",
    "                sess.graph_def, \n",
    "        #             [\"l1/out\",'l2/out'])\n",
    "                [\"l1/out\"])\n",
    "\n",
    "#         x1 = tf.placeholder(tf.float32, name=\"input_1\")\n",
    "#         x2 = tf.placeholder(tf.float32, name='input_2')\n",
    "#         out1 = tf.import_graph_def(g1def,input_map={\"Placeholder:0\": x1}, return_elements=[\"l1/out:0\"])[0]\n",
    "#         out2 = tf.import_graph_def(g2def,input_map={\"Placeholder:0\": x2}, return_elements=[\"l1/out:0\"])[0]\n",
    "#         print(out1)\n",
    "#         print(out2)\n",
    "# #             pdb.set_trace()\n",
    "#         with tf.name_scope('final_layer') as scope:\n",
    "#             w = tf.Variable(tf.random_normal([10, 1]),name='w')\n",
    "#             b = tf.Variable(tf.random_normal([1]),name='b')\n",
    "#             final_out = tf.nn.sigmoid(tf.matmul(tf.concat([out1,out2],1), w) + b, name='final_out')\n",
    "\n",
    "#         saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "#         print(sess.run(out1,{x1:for_input,x2:for_input}))\n",
    "#         print((sess.graph.get_tensor_by_name('import/l1/w:0')).eval())\n",
    "#         print((sess.graph.get_tensor_by_name('import_1/l1/w:0')).eval())\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#         save_path = saver.save(sess, './log/comb/main.ckpt')\n",
    "#         print(\"Model saved in path: %s\" % './log/comb/main.ckpt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./log/model_woody_trial.ckpt\n",
      "INFO:tensorflow:Froze 42 variables.\n",
      "INFO:tensorflow:Converted 42 variables to const ops.\n",
      "Tensor(\"input_woody/keep_prob:0\", dtype=float32)\n",
      "Tensor(\"input_woody/is_training_forBN:0\", dtype=bool)\n",
      "INFO:tensorflow:Restoring parameters from ./log/model_woody_trial.ckpt\n",
      "INFO:tensorflow:Froze 42 variables.\n",
      "INFO:tensorflow:Converted 42 variables to const ops.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of node import/IteratorFromStringHandleV2 was passed float from input_X:0 incompatible with expected string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/vpython/vp3.5.2/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    417\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 418\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input 0 of node import/IteratorFromStringHandleV2 was passed float from input_X:0 incompatible with expected string.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e5c47db3ce49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# #             print(\"Model saved in path: %s\" % './log/comb/main.ckpt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtest_comb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e5c47db3ce49>\u001b[0m in \u001b[0;36mtest_comb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#                                                        'input_woody/keep_prob:0':keep_prob,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#                                                       'input_woody/is_training_forBN:0':is_training_forBN},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                        return_elements=[\"l7_fc/fc_out2:0\"])[0]\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m#             out2 = tf.import_graph_def(gd_2,input_map={\"input_woody/input_X:0\": X,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#                                                        'input_woody/keep_prob:0':keep_prob,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vpython/vp3.5.2/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/vpython/vp3.5.2/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    420\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of node import/IteratorFromStringHandleV2 was passed float from input_X:0 incompatible with expected string."
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "def test_comb():    \n",
    "    with tf.Graph().as_default() as graph_1: \n",
    "        with tf.Session(graph=graph_1) as sess: \n",
    "            saver = tf.train.import_meta_graph('./log/model_woody_trial.ckpt.meta') \n",
    "            saver.restore(sess, './log/model_woody_trial.ckpt')\n",
    "            gd_1 = graph_util.convert_variables_to_constants( \n",
    "                sess, \n",
    "                sess.graph_def, \n",
    "                [\"l7_fc/fc_out2\"])\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "            print(sess.graph.get_tensor_by_name('input_woody/keep_prob:0'))\n",
    "            print(sess.graph.get_tensor_by_name('input_woody/is_training_forBN:0'))\n",
    "            \n",
    "    with tf.Graph().as_default() as graph_2: \n",
    "        with tf.Session(graph=graph_2) as sess: \n",
    "            saver = tf.train.import_meta_graph('./log/model_woody_trial.ckpt.meta') \n",
    "            saver.restore(sess, './log/model_woody_trial.ckpt')\n",
    "            gd_2 = graph_util.convert_variables_to_constants( \n",
    "                sess, \n",
    "                sess.graph_def, \n",
    "                [\"l7_fc/fc_out2\"])\n",
    "\n",
    "    \n",
    "    with tf.Graph().as_default() as graph_comb: \n",
    "        with tf.Session(graph=graph_comb) as sess: \n",
    "            X = tf.reshape(np.random.rand(67,67,67).astype(np.float32), [-1,67,67,67,1],name='input_X')\n",
    "            keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            is_training_forBN = tf.placeholder(tf.bool, name='is_training_forBN')\n",
    "            \n",
    "#             out1 = tf.import_graph_def(gd_1,input_map={\"Placeholder:0\": X},\n",
    "# #                                                        'input_woody/keep_prob:0':keep_prob,\n",
    "# #                                                       'input_woody/is_training_forBN:0':is_training_forBN}, \n",
    "#                                        return_elements=[\"l7_fc/fc_out2:0\"])[0]\n",
    "#             out2 = tf.import_graph_def(gd_2,input_map={\"input_woody/input_X:0\": X,\n",
    "#                                                        'input_woody/keep_prob:0':keep_prob,\n",
    "#                                                       'input_woody/is_training_forBN:0':is_training_forBN}, \n",
    "#                                        return_elements=[\"l7_fc/fc_out2:0\"])[0]\n",
    "#             print(out1)\n",
    "#             print(out2)\n",
    "# #             pdb.set_trace()\n",
    "#             with tf.name_scope('final_layer') as scope:\n",
    "#                 w = tf.Variable(tf.random_normal([256, 1]),name='w')\n",
    "#                 b = tf.Variable(tf.random_normal([1]),name='b')\n",
    "#                 final_out = tf.nn.sigmoid(tf.matmul(tf.concat([out1,out2],1), w) + b, name='final_out')\n",
    "\n",
    "#             saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "#             writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "# #             print((sess.graph.get_tensor_by_name('import/l1/w:0')).eval())\n",
    "# #             print((sess.graph.get_tensor_by_name('import_1/l1/w:0')).eval())\n",
    "# #             sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "# #             save_path = saver.save(sess, './log/comb/main.ckpt')\n",
    "# #             print(\"Model saved in path: %s\" % './log/comb/main.ckpt')\n",
    "\n",
    "test_comb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mW0512 17:56:31.146743 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.146743 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.184132 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.184132 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.254689 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.254689 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "TensorBoard 1.11.0 at http://woody-System-Product-Name:6006 (Press CTRL+C to quit)\n",
      "\u001b[33mW0512 17:56:31.328341 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.328341 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.386453 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.386452 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.439846 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.439846 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.489187 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.489187 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.523646 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.523646 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.548275 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.548274 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.563521 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.563520 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.578003 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.578002 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.592689 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.592689 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.607130 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.607130 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.621642 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.621642 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.636853 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.636853 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.657984 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.657984 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.663459 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.663459 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.667731 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.667731 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.668029 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.668029 140324956325632 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mW0512 17:56:31.687232 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.687232 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.687434 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.687434 140324956325632 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.700718 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.700718 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.700917 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.700917 140324956325632 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.710157 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.710156 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.710356 Reloader tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.710356 140324956325632 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.720825 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.720825 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.724274 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.724274 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.728518 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.728518 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.734858 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.734858 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.738415 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.738415 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.741453 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.741453 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.745233 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.745233 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.753782 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.753782 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.756991 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.756991 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.760936 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.760936 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.765112 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.765111 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.772288 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.772288 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.776783 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.776783 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.781103 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.781102 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.785159 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.785159 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[33mW0512 17:56:31.789294 Reloader tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0mW0512 17:56:31.789294 140324956325632 tf_logging.py:120] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mW0512 17:56:34.139275 Thread-1 application.py:300] path /[[getImageSrc(feat.name)]] not found, sending 404\n",
      "\u001b[0mW0512 17:56:34.139275 140324947670784 application.py:300] path /[[getImageSrc(feat.name)]] not found, sending 404\n",
      "\u001b[33mW0512 17:56:34.140035 Thread-2 application.py:300] path /[[getCompareImageSrc(feat.name)]] not found, sending 404\n",
      "\u001b[0mW0512 17:56:34.140035 140324939278080 application.py:300] path /[[getCompareImageSrc(feat.name)]] not found, sending 404\n",
      "\u001b[33mW0512 17:56:34.140461 Thread-3 application.py:300] path /[[getSeqImageSrc(seqfeat.name, seqNumber)]] not found, sending 404\n",
      "\u001b[0mW0512 17:56:34.140461 140324930885376 application.py:300] path /[[getSeqImageSrc(seqfeat.name, seqNumber)]] not found, sending 404\n",
      "\u001b[33mW0512 17:56:34.140748 Thread-4 application.py:300] path /[[getCompareSeqImageSrc(seqfeat.name, seqNumber)]] not found, sending 404\n",
      "\u001b[0mW0512 17:56:34.140748 140324922492672 application.py:300] path /[[getCompareSeqImageSrc(seqfeat.name, seqNumber)]] not found, sending 404\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(67,67,67).astype(np.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ndarray.astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_net(x):\n",
    "    with tf.name_scope('l1') as scope:\n",
    "        \n",
    "        x_shape = x.get_shape().as_list()\n",
    "        w = tf.Variable(tf.random_normal([x_shape[1], 5]),name='w')\n",
    "        b = tf.Variable(tf.random_normal([5]),name='b')\n",
    "        out = tf.nn.sigmoid(tf.matmul(x, w) + b,name='out')\n",
    "    with tf.name_scope('l2') as scope:\n",
    "        w = tf.Variable(tf.random_normal([5, 1]),name='w')\n",
    "        b = tf.Variable(tf.random_normal([1]),name='b')\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, w) + b,name='out')\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_model(log_file):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.name_scope(log_file) as scope:\n",
    "        input1 = tf.placeholder(tf.float32, [None, 2])\n",
    "        p1=tf.placeholder(tf.float32, [2],name='p1')\n",
    "        p2=tf.placeholder(tf.float32, [2],name='p2')\n",
    "        output1 = bb_net(input1) # 2-20-3 network\n",
    "        #     output1 = bb_net_rename(input1)\n",
    "        print(output1.shape)\n",
    "\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "    #     sess.run(output1,feed_dict={input1:np.random.rand(3,2)})\n",
    "        var = tf.global_variables()\n",
    "#         pdb.set_trace()\n",
    "        save_path = saver.save(sess, './log/'+log_file+'.ckpt')\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "    return\n",
    "\n",
    "save_model(\"save_1\")\n",
    "save_model(\"save_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_net(x):\n",
    "    with tf.name_scope('l1') as scope:\n",
    "        x_shape = x.get_shape().as_list()\n",
    "        w = tf.Variable(tf.random_normal([x_shape[1], 5]),name='w')\n",
    "        b = tf.Variable(tf.random_normal([5]),name='b')\n",
    "        out = tf.nn.sigmoid(tf.matmul(x, w) + b,name='out')\n",
    "    with tf.name_scope('l2') as scope:\n",
    "        w = tf.Variable(tf.random_normal([5, 1]),name='w')\n",
    "        b = tf.Variable(tf.random_normal([1]),name='b')\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, w) + b,name='out')\n",
    "    return out\n",
    "\n",
    "with tf.Graph().as_default() as g_comb: \n",
    "    with tf.Session(graph=g_comb) as sess: \n",
    "        with tf.name_scope('save_1') as scope:\n",
    "            input1 = tf.placeholder(tf.float32, [None, 2])\n",
    "            p1=tf.placeholder(tf.float32, [2],name='p1')\n",
    "            p2=tf.placeholder(tf.float32, [2],name='p2')\n",
    "            output1 = bb_net(input1) # 2-20-3 network\n",
    "            output2 = output1\n",
    "\n",
    "            saver = tf.train.Saver([p_name for p_name in tf.global_variables() if 'save_1' in p_name.name])\n",
    "            saver.restore(sess, './log/save_1.ckpt')\n",
    "        with tf.name_scope('save_2') as scope:\n",
    "            input1 = tf.placeholder(tf.float32, [None, 2])\n",
    "            p1=tf.placeholder(tf.float32, [2],name='p1')\n",
    "            p2=tf.placeholder(tf.float32, [2],name='p2')\n",
    "            output1 = bb_net(input1) # 2-20-3 network\n",
    "            \n",
    "            saver = tf.train.Saver([p_name for p_name in tf.global_variables() if 'save_2' in p_name.name])\n",
    "            \n",
    "            saver.restore(sess, './log/save_2.ckpt')\n",
    "#        \n",
    "        \n",
    "        writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_1/l1/w:0'][0].eval())\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_1/l1/b:0'][0].eval())\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_1/l2/w:0'][0].eval())\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_1/l2/b:0'][0].eval())\n",
    "        \n",
    "        print([w for w in tf.global_variables() if w.name == 'save_2/l1/w:0'][0].eval())\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_2/l1/b:0'][0].eval())\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_2/l2/w:0'][0].eval())\n",
    "        print([w for w in tf.global_variables() if w.name == 'save_2/l2/b:0'][0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[-0.10215719 -1.0839691   0.9889877   0.56713724 -1.1945558 ]\n",
    " [-0.4956044   0.4283593   1.1947318   0.64011204  1.0792035 ]]\n",
    "[-0.00222475  2.073958   -0.36912346  0.18108483  0.221729  ]\n",
    "[[ 1.746935  ]\n",
    " [-0.52356195]\n",
    " [-0.86633795]\n",
    " [ 1.1020536 ]\n",
    " [-0.11223865]]\n",
    "[-0.4241702]\n",
    "\n",
    "[[ 1.1553413  -1.1046942   0.24588911 -1.5334411  -0.2820287 ]\n",
    " [-0.55618083 -0.48955762 -0.5473364  -0.803786   -1.0038443 ]]\n",
    "[-3.3585887  -0.37282467 -0.52793515  0.50497735  1.015645  ]\n",
    "[[0.6794571 ]\n",
    " [1.2335973 ]\n",
    " [0.6630942 ]\n",
    " [0.19118617]\n",
    " [0.7129952 ]]\n",
    "[0.17231368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(): \n",
    "    with tf.Session() as sess: \n",
    "        with tf.name_scope('alff'):\n",
    "            saver = tf.train.import_meta_graph('./log/save_1.ckpt.meta') \n",
    "            saver.restore(sess, './log/save_1.ckpt')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess,'./log/save_1_alff.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ckpt(\"./log/save_1.ckpt\")\n",
    "show_ckpt(\"./log/save_1_alff.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../nki_raw/log/model_mse_woody.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../nki_raw/log/model_person_woody.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../nki_alff/log/model_mse_woody.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../nki_alff/log/model_person_woody.ckpt\n"
     ]
    }
   ],
   "source": [
    "# trial_1 \n",
    "# to save raw and alff with new name scope\n",
    "\n",
    "def resave(input_saver,output_saver,scope_name):\n",
    "    '''\n",
    "    input_saver: str, .ckpt file path\n",
    "    output_saver: str, .ckpt file path\n",
    "    scope_name: str\n",
    "    '''\n",
    "    with tf.Graph().as_default(): \n",
    "        with tf.Session() as sess: \n",
    "            with tf.variable_scope(scope_name):\n",
    "                saver = tf.train.import_meta_graph(input_saver+'.meta') \n",
    "                saver.restore(sess, input_saver)\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess,output_saver)\n",
    "\n",
    "resave('../nki_raw/log/model_mse_woody.ckpt', './log/model_mse_woody_raw.ckpt','raw')\n",
    "resave('../nki_raw/log/model_person_woody.ckpt', './log/model_person_woody_raw.ckpt','raw')\n",
    "resave('../nki_alff/log/model_mse_woody.ckpt', './log/model_mse_woody_alff.ckpt','alff')\n",
    "resave('../nki_alff/log/model_person_woody.ckpt', './log/model_person_woody_alff.ckpt','alff')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- - ----------------------------------------\n",
      "tensor_name:  l7_fc/w/Adam\n",
      "tensor_name:  batch_normalization_2/moving_mean\n",
      "tensor_name:  batch_normalization_5/beta/Adam_1\n",
      "tensor_name:  l3_conv3d/b\n",
      "tensor_name:  l7_fc/b/Adam_1\n",
      "tensor_name:  l3_conv3d/kernel/Adam\n",
      "tensor_name:  l2_conv3d/kernel/Adam_1\n",
      "tensor_name:  batch_normalization_3/moving_variance\n",
      "tensor_name:  l1_conv3d/kernel\n",
      "tensor_name:  batch_normalization_2/beta/Adam_1\n",
      "tensor_name:  beta1_power\n",
      "tensor_name:  batch_normalization_4/beta\n",
      "tensor_name:  l2_conv3d/kernel/Adam\n",
      "tensor_name:  batch_normalization_6/moving_variance\n",
      "tensor_name:  l5_conv3d/kernel/Adam\n",
      "tensor_name:  l1_conv3d/kernel/Adam_1\n",
      "tensor_name:  batch_normalization_1/beta/Adam\n",
      "tensor_name:  l6_fc/b/Adam\n",
      "tensor_name:  l1_conv3d/b/Adam\n",
      "tensor_name:  l7_fc/b/Adam\n",
      "tensor_name:  l4_conv3d/kernel/Adam_1\n",
      "tensor_name:  batch_normalization_2/gamma/Adam\n",
      "tensor_name:  l5_conv3d/b/Adam_1\n",
      "tensor_name:  batch_normalization_4/beta/Adam\n",
      "tensor_name:  batch_normalization/beta/Adam\n",
      "tensor_name:  batch_normalization_5/beta\n",
      "tensor_name:  batch_normalization_3/beta/Adam_1\n",
      "tensor_name:  l2_conv3d/b/Adam_1\n",
      "tensor_name:  batch_normalization_2/gamma\n",
      "tensor_name:  l4_conv3d/b/Adam\n",
      "tensor_name:  batch_normalization_3/gamma\n",
      "tensor_name:  l7_fc/w/Adam_1\n",
      "tensor_name:  batch_normalization_2/beta\n",
      "tensor_name:  batch_normalization_1/moving_variance\n",
      "tensor_name:  l1_conv3d/b\n",
      "tensor_name:  batch_normalization_3/gamma/Adam\n",
      "tensor_name:  batch_normalization_3/beta\n",
      "tensor_name:  l4_conv3d/b\n",
      "tensor_name:  l6_fc/b\n",
      "tensor_name:  batch_normalization_6/moving_mean\n",
      "tensor_name:  batch_normalization_6/beta/Adam_1\n",
      "tensor_name:  l1_conv3d/b/Adam_1\n",
      "tensor_name:  l8_fc/b/Adam\n",
      "tensor_name:  l6_fc/w\n",
      "tensor_name:  l4_conv3d/kernel/Adam\n",
      "tensor_name:  batch_normalization_4/moving_variance\n",
      "tensor_name:  batch_normalization_4/gamma\n",
      "tensor_name:  batch_normalization/gamma/Adam\n",
      "tensor_name:  batch_normalization_4/gamma/Adam_1\n",
      "tensor_name:  l6_fc/w/Adam_1\n",
      "tensor_name:  l6_fc/w/Adam\n",
      "tensor_name:  batch_normalization_2/beta/Adam\n",
      "tensor_name:  batch_normalization/gamma/Adam_1\n",
      "tensor_name:  batch_normalization/gamma\n",
      "tensor_name:  batch_normalization/moving_mean\n",
      "tensor_name:  batch_normalization_2/gamma/Adam_1\n",
      "tensor_name:  l7_fc/w\n",
      "tensor_name:  l8_fc/w/Adam\n",
      "tensor_name:  batch_normalization/beta\n",
      "tensor_name:  l5_conv3d/b/Adam\n",
      "tensor_name:  batch_normalization_4/gamma/Adam\n",
      "tensor_name:  l8_fc/b/Adam_1\n",
      "tensor_name:  batch_normalization_5/gamma/Adam_1\n",
      "tensor_name:  batch_normalization_1/gamma\n",
      "tensor_name:  l8_fc/b\n",
      "tensor_name:  batch_normalization_5/moving_mean\n",
      "tensor_name:  batch_normalization_6/beta\n",
      "tensor_name:  batch_normalization_1/moving_mean\n",
      "tensor_name:  l2_conv3d/b/Adam\n",
      "tensor_name:  l4_conv3d/kernel\n",
      "tensor_name:  l7_fc/b\n",
      "tensor_name:  batch_normalization_4/beta/Adam_1\n",
      "tensor_name:  batch_normalization_6/gamma/Adam_1\n",
      "tensor_name:  beta2_power\n",
      "tensor_name:  l5_conv3d/b\n",
      "tensor_name:  l3_conv3d/kernel/Adam_1\n",
      "tensor_name:  batch_normalization_3/gamma/Adam_1\n",
      "tensor_name:  batch_normalization_3/moving_mean\n",
      "tensor_name:  l8_fc/w\n",
      "tensor_name:  batch_normalization/moving_variance\n",
      "tensor_name:  l3_conv3d/b/Adam_1\n",
      "tensor_name:  l5_conv3d/kernel/Adam_1\n",
      "tensor_name:  l5_conv3d/kernel\n",
      "tensor_name:  batch_normalization_5/moving_variance\n",
      "tensor_name:  batch_normalization_1/beta/Adam_1\n",
      "tensor_name:  batch_normalization_4/moving_mean\n",
      "tensor_name:  l3_conv3d/b/Adam\n",
      "tensor_name:  batch_normalization_5/beta/Adam\n",
      "tensor_name:  batch_normalization_1/beta\n",
      "tensor_name:  l6_fc/b/Adam_1\n",
      "tensor_name:  batch_normalization_1/gamma/Adam\n",
      "tensor_name:  batch_normalization/beta/Adam_1\n",
      "tensor_name:  l3_conv3d/kernel\n",
      "tensor_name:  l2_conv3d/b\n",
      "tensor_name:  l4_conv3d/b/Adam_1\n",
      "tensor_name:  batch_normalization_5/gamma/Adam\n",
      "tensor_name:  batch_normalization_6/beta/Adam\n",
      "tensor_name:  batch_normalization_6/gamma/Adam\n",
      "tensor_name:  batch_normalization_6/gamma\n",
      "tensor_name:  batch_normalization_2/moving_variance\n",
      "tensor_name:  batch_normalization_5/gamma\n",
      "tensor_name:  batch_normalization_1/gamma/Adam_1\n",
      "tensor_name:  l2_conv3d/kernel\n",
      "tensor_name:  l1_conv3d/kernel/Adam\n",
      "tensor_name:  l8_fc/w/Adam_1\n",
      "tensor_name:  batch_normalization_3/beta/Adam\n"
     ]
    }
   ],
   "source": [
    "show_ckpt('../nki_raw/log/model_mse_woody.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- - ----------------------------------------\n",
      "tensor_name:  raw/l8_fc/b/Adam_1\n",
      "tensor_name:  raw/batch_normalization_6/beta/Adam\n",
      "tensor_name:  raw/batch_normalization_1/gamma/Adam_1\n",
      "tensor_name:  raw/beta2_power\n",
      "tensor_name:  raw/batch_normalization_3/beta\n",
      "tensor_name:  raw/batch_normalization_6/moving_variance\n",
      "tensor_name:  raw/batch_normalization_6/gamma/Adam_1\n",
      "tensor_name:  raw/batch_normalization_3/beta/Adam_1\n",
      "tensor_name:  raw/batch_normalization_4/gamma/Adam_1\n",
      "tensor_name:  raw/batch_normalization_1/moving_mean\n",
      "tensor_name:  raw/batch_normalization_3/moving_mean\n",
      "tensor_name:  raw/l6_fc/b\n",
      "tensor_name:  raw/batch_normalization_4/beta\n",
      "tensor_name:  raw/l5_conv3d/b\n",
      "tensor_name:  raw/batch_normalization_2/gamma/Adam_1\n",
      "tensor_name:  raw/batch_normalization/gamma/Adam\n",
      "tensor_name:  raw/l4_conv3d/kernel/Adam_1\n",
      "tensor_name:  raw/l1_conv3d/b/Adam\n",
      "tensor_name:  raw/batch_normalization_3/gamma/Adam_1\n",
      "tensor_name:  raw/l4_conv3d/kernel/Adam\n",
      "tensor_name:  raw/batch_normalization_2/gamma/Adam\n",
      "tensor_name:  raw/beta1_power\n",
      "tensor_name:  raw/l6_fc/w/Adam_1\n",
      "tensor_name:  raw/l1_conv3d/b/Adam_1\n",
      "tensor_name:  raw/l6_fc/w\n",
      "tensor_name:  raw/batch_normalization/moving_mean\n",
      "tensor_name:  raw/l5_conv3d/b/Adam\n",
      "tensor_name:  raw/batch_normalization_3/gamma\n",
      "tensor_name:  raw/l4_conv3d/kernel\n",
      "tensor_name:  raw/batch_normalization/moving_variance\n",
      "tensor_name:  raw/l5_conv3d/b/Adam_1\n",
      "tensor_name:  raw/batch_normalization_6/beta/Adam_1\n",
      "tensor_name:  raw/l4_conv3d/b/Adam_1\n",
      "tensor_name:  raw/l5_conv3d/kernel\n",
      "tensor_name:  raw/batch_normalization_4/gamma\n",
      "tensor_name:  raw/l8_fc/b\n",
      "tensor_name:  raw/batch_normalization_3/moving_variance\n",
      "tensor_name:  raw/batch_normalization_2/beta\n",
      "tensor_name:  raw/l2_conv3d/kernel/Adam\n",
      "tensor_name:  raw/batch_normalization_3/beta/Adam\n",
      "tensor_name:  raw/batch_normalization/beta/Adam\n",
      "tensor_name:  raw/l3_conv3d/b/Adam\n",
      "tensor_name:  raw/batch_normalization_1/beta\n",
      "tensor_name:  raw/l5_conv3d/kernel/Adam\n",
      "tensor_name:  raw/batch_normalization_5/beta/Adam_1\n",
      "tensor_name:  raw/batch_normalization_2/beta/Adam_1\n",
      "tensor_name:  raw/batch_normalization_2/moving_mean\n",
      "tensor_name:  raw/batch_normalization_4/gamma/Adam\n",
      "tensor_name:  raw/l6_fc/b/Adam\n",
      "tensor_name:  raw/l3_conv3d/b\n",
      "tensor_name:  raw/l7_fc/w/Adam_1\n",
      "tensor_name:  raw/batch_normalization_5/beta/Adam\n",
      "tensor_name:  raw/batch_normalization/gamma\n",
      "tensor_name:  raw/batch_normalization_1/moving_variance\n",
      "tensor_name:  raw/l7_fc/w\n",
      "tensor_name:  raw/batch_normalization_1/gamma\n",
      "tensor_name:  raw/l2_conv3d/b/Adam\n",
      "tensor_name:  raw/l4_conv3d/b/Adam\n",
      "tensor_name:  raw/batch_normalization_6/gamma\n",
      "tensor_name:  raw/batch_normalization/gamma/Adam_1\n",
      "tensor_name:  raw/batch_normalization_1/gamma/Adam\n",
      "tensor_name:  raw/l8_fc/w/Adam_1\n",
      "tensor_name:  raw/l1_conv3d/kernel\n",
      "tensor_name:  raw/l6_fc/w/Adam\n",
      "tensor_name:  raw/l7_fc/w/Adam\n",
      "tensor_name:  raw/l3_conv3d/kernel\n",
      "tensor_name:  raw/l7_fc/b\n",
      "tensor_name:  raw/l1_conv3d/b\n",
      "tensor_name:  raw/batch_normalization_3/gamma/Adam\n",
      "tensor_name:  raw/l7_fc/b/Adam\n",
      "tensor_name:  raw/batch_normalization_5/gamma/Adam\n",
      "tensor_name:  raw/l3_conv3d/b/Adam_1\n",
      "tensor_name:  raw/batch_normalization_5/beta\n",
      "tensor_name:  raw/batch_normalization_1/beta/Adam_1\n",
      "tensor_name:  raw/batch_normalization_4/beta/Adam\n",
      "tensor_name:  raw/l5_conv3d/kernel/Adam_1\n",
      "tensor_name:  raw/l8_fc/w\n",
      "tensor_name:  raw/l1_conv3d/kernel/Adam_1\n",
      "tensor_name:  raw/batch_normalization_5/moving_mean\n",
      "tensor_name:  raw/l4_conv3d/b\n",
      "tensor_name:  raw/batch_normalization_4/moving_mean\n",
      "tensor_name:  raw/l8_fc/w/Adam\n",
      "tensor_name:  raw/batch_normalization/beta\n",
      "tensor_name:  raw/l2_conv3d/kernel\n",
      "tensor_name:  raw/batch_normalization_2/moving_variance\n",
      "tensor_name:  raw/batch_normalization_5/moving_variance\n",
      "tensor_name:  raw/l2_conv3d/kernel/Adam_1\n",
      "tensor_name:  raw/batch_normalization_4/moving_variance\n",
      "tensor_name:  raw/batch_normalization_5/gamma/Adam_1\n",
      "tensor_name:  raw/l3_conv3d/kernel/Adam\n",
      "tensor_name:  raw/batch_normalization_4/beta/Adam_1\n",
      "tensor_name:  raw/l3_conv3d/kernel/Adam_1\n",
      "tensor_name:  raw/batch_normalization_5/gamma\n",
      "tensor_name:  raw/batch_normalization_6/beta\n",
      "tensor_name:  raw/batch_normalization_1/beta/Adam\n",
      "tensor_name:  raw/batch_normalization/beta/Adam_1\n",
      "tensor_name:  raw/batch_normalization_6/moving_mean\n",
      "tensor_name:  raw/batch_normalization_2/gamma\n",
      "tensor_name:  raw/l8_fc/b/Adam\n",
      "tensor_name:  raw/l6_fc/b/Adam_1\n",
      "tensor_name:  raw/batch_normalization_2/beta/Adam\n",
      "tensor_name:  raw/l1_conv3d/kernel/Adam\n",
      "tensor_name:  raw/l2_conv3d/b\n",
      "tensor_name:  raw/l2_conv3d/b/Adam_1\n",
      "tensor_name:  raw/l7_fc/b/Adam_1\n",
      "tensor_name:  raw/batch_normalization_6/gamma/Adam\n"
     ]
    }
   ],
   "source": [
    "show_ckpt('./log/model_mse_woody_raw.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./log/model_mse_woody_raw.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./log/model_mse_woody_alff.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |   #                                             | 100 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check finished.\n",
      "[<tf.Variable 'last_layer/w:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'last_layer/b:0' shape=(1,) dtype=float32_ref>]\n",
      "[<tf.Variable 'last_layer/w:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'last_layer/b:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'last_layer/w/Adam:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'last_layer/w/Adam_1:0' shape=(256, 1) dtype=float32_ref>, <tf.Variable 'last_layer/b/Adam:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'last_layer/b/Adam_1:0' shape=(1,) dtype=float32_ref>]\n",
      "0.086353175 [0.1]\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# to restore two networks\n",
    "from progressbar import *\n",
    "\n",
    "sizeof_kernel_1 = 16\n",
    "root_dir = './'\n",
    "FLAGS_batch_size = 10\n",
    "# arr = np.load('../nki_/data_npy/mean_npy.npy')\n",
    "FLAGS_arr_shape_raw = (67,67,67)\n",
    "\n",
    "FLAGS_arr_shape_alff = np.load('../nki_alff/data_npy/mean_npy.npy').shape\n",
    "\n",
    "\n",
    "\n",
    "def print_activations(t):\n",
    "    print(t.op.name, ' ', t.get_shape().as_list())\n",
    "\n",
    "def inference4comb(X, keep_prob=1.0, is_training_forBN=False, trivial=True, FLAGS_arr_shape=FLAGS_arr_shape_raw):\n",
    "    l2_loss = 0\n",
    "    with tf.name_scope('l1_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([5,5,5,1,sizeof_kernel_1], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[sizeof_kernel_1]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(X,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l2_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,sizeof_kernel_1,32], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[32]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l3_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,32,64], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[64]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l4_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,64,64], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[64]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l5_conv3d') as scope:  # temp\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,64,64], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[64]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l6_fc') as scope:\n",
    "        max_pool_shape = max_pool.get_shape().as_list()\n",
    "        temp_shape = 1\n",
    "        for i in max_pool_shape[1:]:\n",
    "            temp_shape *= i\n",
    "        fc_input = tf.reshape(max_pool, [-1, temp_shape])\n",
    "        w = tf.Variable(tf.truncated_normal([temp_shape,512],stddev=0.1),name='w')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[512]),name='b')\n",
    "        temp_output = tf.matmul(fc_input,w) + b\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        fc_out = tf.nn.relu(temp_output, name='fc_out1')\n",
    "        dropout = tf.nn.dropout(fc_out,keep_prob=keep_prob, name='dropout1')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(fc_out)\n",
    "            print_activations(dropout)\n",
    "    with tf.name_scope('l7_fc') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([512,128],stddev=0.1),name='w')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[128]),name='b')\n",
    "        temp_output = tf.matmul(fc_out,w) + b\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        fc_out = tf.nn.relu(temp_output, name='fc_out2')\n",
    "        \n",
    "#         print(fc_out.name, id(fc_out))\n",
    "        dropout = tf.nn.dropout(fc_out,keep_prob=keep_prob, name='dropout2')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(fc_out)\n",
    "            print_activations(dropout)\n",
    "    with tf.name_scope('l8_fc') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([128,1],stddev=0.1),name='w')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[1]),name='b')\n",
    "        \n",
    "#         print(b.name,id(b))\n",
    "        \n",
    "        final_output = tf.add(tf.matmul(dropout,w), b, name='final_output')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(w)\n",
    "\n",
    "        if trivial:\n",
    "            print_activations(final_output)\n",
    "    \n",
    "    return final_output, l2_loss, fc_out\n",
    "        \n",
    "def get_loss(predict_batches,label_batches):\n",
    "    '''\n",
    "    we are not sure the shape of predict_batches,label_batches, (?,1) or (?,),\n",
    "    so we reshape them into (?,) first.\n",
    "    '''\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        predict_batches = tf.reshape(predict_batches,[-1,1])\n",
    "        label_batches = tf.reshape(label_batches,[-1,1])\n",
    "        cost = tf.reduce_mean(tf.square(predict_batches - label_batches))\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "def decode(serialized_example):\n",
    "    '''\n",
    "    serialized_example: tf.data.TFRecordDataset\n",
    "    '''\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "            'label': tf.FixedLenFeature([],tf.float32),\n",
    "            'id': tf.FixedLenFeature([],tf.int64),\n",
    "        }\n",
    "    )\n",
    "    arr = tf.decode_raw(features['arr_raw'],tf.float32)\n",
    "    arr = tf.reshape(arr,list(FLAGS_arr_shape))\n",
    "    label = features['label']\n",
    "    sub_id = features['id']\n",
    "    \n",
    "    return arr,label,sub_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_iterator(for_training=True,num_epochs=1):\n",
    "    if not num_epochs:\n",
    "        num_epochs = None\n",
    "    filename = os.path.join(root_dir,('training_data' if for_training else 'test_data')+'.tfrec')\n",
    "    with tf.name_scope('input'):\n",
    "        dataset = tf.data.TFRecordDataset(filename)\n",
    "#         pdb.set_trace()\n",
    "        dataset = dataset.map(decode)\n",
    "        if for_training:\n",
    "            dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(FLAGS_batch_size)\n",
    "        if for_training:\n",
    "            train_iterator = dataset.make_one_shot_iterator()\n",
    "            val_iterator = dataset.make_one_shot_iterator()\n",
    "            iterators = [train_iterator,val_iterator]\n",
    "        else:\n",
    "            test_iterator = dataset.make_initializable_iterator()\n",
    "            iterators = [test_iterator]\n",
    "\n",
    "        \n",
    "    return iterators\n",
    "\n",
    "\n",
    "\n",
    "def test_load(model_path='./'):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            with tf.variable_scope('raw'):\n",
    "                input_iterator = get_iterator(for_training=False)[0]\n",
    "                handle = tf.placeholder(tf.string,shape=[])\n",
    "                iterator = tf.data.Iterator.from_string_handle(handle, input_iterator.output_types)\n",
    "                arr_batch,label_batch,id_batch = iterator.get_next()\n",
    "                X = tf.reshape(arr_batch, [-1]+list(FLAGS_arr_shape_raw)+[1])\n",
    "                \n",
    "\n",
    "                _,_,fc_out_raw = inference4comb(X,trivial=False)\n",
    "\n",
    "                \n",
    "\n",
    "                saver = tf.train.Saver([p_name for p_name in tf.global_variables() if 'raw/' in p_name.name])\n",
    "                saver.restore(sess, './log/model_mse_woody_raw.ckpt')\n",
    "\n",
    "            with tf.variable_scope('alff'):\n",
    "                input_iterator = get_iterator(for_training=False)[0]\n",
    "                handle = tf.placeholder(tf.string,shape=[])\n",
    "                iterator = tf.data.Iterator.from_string_handle(handle, input_iterator.output_types)\n",
    "                arr_batch,label_batch,id_batch = iterator.get_next()\n",
    "                X = tf.reshape(arr_batch, [-1]+list(FLAGS_arr_shape_alff)+[1])\n",
    "\n",
    "                _,_,fc_out_alff = inference4comb(X,trivial=False,\n",
    "                                                  FLAGS_arr_shape=FLAGS_arr_shape_alff)\n",
    "\n",
    "\n",
    "                saver = tf.train.Saver([p_name for p_name in tf.global_variables() if 'alff/' in p_name.name])\n",
    "                saver.restore(sess, './log/model_mse_woody_alff.ckpt')\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            param_list = tf.global_variables()\n",
    "            reader_raw = pywrap_tensorflow.NewCheckpointReader('./log/model_mse_woody_raw.ckpt')\n",
    "            reader_alff = pywrap_tensorflow.NewCheckpointReader('./log/model_mse_woody_alff.ckpt')\n",
    "            \n",
    "            pbar = ProgressBar().start()\n",
    "            n_bar = len(param_list)\n",
    "            for i,param in enumerate(param_list):\n",
    "                if 'alff/' in param.name:\n",
    "                    if np.sum(sess.graph.get_tensor_by_name(param.name).eval() - \n",
    "                          reader_alff.get_tensor(param.name.split(':')[0])) != 0:\n",
    "                        print(param)\n",
    "                elif 'raw/' in param.name:\n",
    "                    if np.sum(sess.graph.get_tensor_by_name(param.name).eval() - \n",
    "                          reader_raw.get_tensor(param.name.split(':')[0])) != 0:\n",
    "                        print(param)\n",
    "                else:\n",
    "                    print('unknown variable: ',param)\n",
    "#                 break\n",
    "                pbar.update(int(i*100/(n_bar-1)))\n",
    "            pbar.finish()\n",
    "            print('Check finished.')\n",
    "#             test_param = [i for i in tf.global_variables() if i.name == 'raw/l8_fc/b:0'][0]\n",
    "#             print(id(test_param))\n",
    "\n",
    "\n",
    "\n",
    "            with tf.name_scope('last_layer'):\n",
    "                w = tf.Variable(tf.truncated_normal([256,1],stddev=0.1),name='w')\n",
    "                b = tf.Variable(tf.constant(0.1,shape=[1]),name='b')\n",
    "\n",
    "                final_output = tf.add(tf.matmul(tf.concat([fc_out_raw,fc_out_alff],1),w), b, name='final_output')\n",
    "            \n",
    "            loss = get_loss(final_output,label_batch)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "\n",
    "            train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                     \"last_layer\")\n",
    "            print(train_vars)\n",
    "            train_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "#             second_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "#                                                   \"scope/prefix/for/second/vars\")                     \n",
    "#             second_train_op = optimizer.minimize(cost, var_list=second_train_vars)\n",
    "            \n",
    "            uninitialized_vars = []\n",
    "            for var in tf.global_variables():\n",
    "                try:\n",
    "                    sess.run(var)\n",
    "                except tf.errors.FailedPreconditionError:\n",
    "                    uninitialized_vars.append(var)\n",
    "            print(uninitialized_vars)\n",
    "            initialize_op = tf.variables_initializer(uninitialized_vars)\n",
    "            sess.run(initialize_op)\n",
    "            print(w.eval()[0,0],b.eval())\n",
    "            print([i for i in tf.global_variables() if i.name=='beta1_power:0'][0].eval())\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "    return \n",
    "\n",
    "test_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- - ----------------------------------------\n",
      "tensor_name:  raw/l8_fc/b/Adam_1\n",
      "value:  [150.37914]\n"
     ]
    }
   ],
   "source": [
    "# to testify the variables:\n",
    "reader = pywrap_tensorflow.NewCheckpointReader('./log/model_mse_woody_raw.ckpt') \n",
    "var_to_shape_map = reader.get_variable_to_shape_map() \n",
    "#     print(var_to_shape_map)\n",
    "print_sep()\n",
    "for key in var_to_shape_map: \n",
    "    print(\"tensor_name: \", key)\n",
    "    print('value: ', reader.get_tensor(key))\n",
    "    break\n",
    "# chkp.print_tensors_in_checkpoint_file('./log/model_mse_woody_raw.ckpt', tensor_name='', all_tensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.batch_normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh: 0: getcwd() failed: Input/output error\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
