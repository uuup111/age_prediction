{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of Ruyi's code.\n",
    "\n",
    "Dataset: IXI\n",
    "\n",
    "10% for test, 90% for training. (Option: k-folds cross validation, not implemented yet.)\n",
    "\n",
    "Using 3D-CNN model code of my own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main:\n",
    "!python preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** using my 3d cnn model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocess import *\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def print_activations(t):\n",
    "    print(t.op.name, ' ', t.get_shape().as_list())\n",
    "\n",
    "def inference(X, keep_prob, is_training_forBN, trivial=True):\n",
    "    l2_loss = 0\n",
    "    with tf.name_scope('l1_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([5,5,5,1,FLAGS.sizeof_kernel_1], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[FLAGS.sizeof_kernel_1]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(X,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l2_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,FLAGS.sizeof_kernel_1,32], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[32]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l3_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,32,64], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[64]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l4_conv3d') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,64,64], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[64]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l5_conv3d') as scope:  # temp\n",
    "        w = tf.Variable(tf.truncated_normal([3,3,3,64,64], stddev=0.1), name='kernel')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[64]),name='b')\n",
    "        temp_output = tf.nn.bias_add(tf.nn.conv3d(max_pool,w,strides=[1,1,1,1,1],\\\n",
    "                                                        padding='SAME',name='conv3d'),b)\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        conv3d = tf.nn.relu(temp_output)\n",
    "        \n",
    "        max_pool = tf.nn.max_pool3d(conv3d,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],\\\n",
    "                                   padding='SAME',name='max_pool3d')\n",
    "        \n",
    "#         l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(conv3d)\n",
    "            print_activations(max_pool)\n",
    "    with tf.name_scope('l6_fc') as scope:\n",
    "        max_pool_shape = max_pool.get_shape().as_list()\n",
    "        temp_shape = 1\n",
    "        for i in max_pool_shape[1:]:\n",
    "            temp_shape *= i\n",
    "        fc_input = tf.reshape(max_pool, [-1, temp_shape])\n",
    "        w = tf.Variable(tf.truncated_normal([temp_shape,512],stddev=0.1),name='w')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[512]),name='b')\n",
    "        temp_output = tf.matmul(fc_input,w) + b\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        fc_out = tf.nn.relu(temp_output, name='fc_out1')\n",
    "        dropout = tf.nn.dropout(fc_out,keep_prob=keep_prob, name='dropout1')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(fc_out)\n",
    "            print_activations(dropout)\n",
    "    with tf.name_scope('l7_fc') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([512,128],stddev=0.1),name='w')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[128]),name='b')\n",
    "        temp_output = tf.matmul(fc_out,w) + b\n",
    "        temp_output = tf.layers.batch_normalization(temp_output,training=is_training_forBN)\n",
    "        fc_out = tf.nn.relu(temp_output, name='fc_out2')\n",
    "        dropout = tf.nn.dropout(fc_out,keep_prob=keep_prob, name='dropout2')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(w)\n",
    "        \n",
    "        if trivial:\n",
    "            print_activations(fc_out)\n",
    "            print_activations(dropout)\n",
    "    with tf.name_scope('l8_fc') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal([128,1],stddev=0.1),name='w')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[1]),name='b')\n",
    "        \n",
    "        final_output = tf.add(tf.matmul(dropout,w), b, name='final_output')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(w)\n",
    "\n",
    "        if trivial:\n",
    "            print_activations(final_output)\n",
    "    \n",
    "    return final_output, l2_loss\n",
    "        \n",
    "def get_loss(predict_batches,label_batches,l2_loss):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cost = tf.reduce_mean(tf.square(predict_batches - label_batches)) + FLAGS.l2_epsilon * l2_loss\n",
    "    return cost\n",
    "\n",
    "def get_accuracy(predicts,labels):\n",
    "    with tf.name_scope('acc'):\n",
    "#         acc = tf.contrib.metrics.streaming_pearson_correlation(predicts, tf.cast(labels,tf.float32))[0]\n",
    "        acc = tf.constant(0)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def trainning(loss):\n",
    "    '''\n",
    "    The weird things are for Batch Normalization.\n",
    "    '''\n",
    "#     train_op = tf.train.RMSPropOptimizer(lr,0.9).minimize(loss)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    return train_op      \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def decode(serialized_example):\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'arr_raw':tf.FixedLenFeature([],tf.string),\n",
    "            'label': tf.FixedLenFeature([],tf.float32),\n",
    "        }\n",
    "    )\n",
    "    arr = tf.decode_raw(features['arr_raw'],tf.float32)\n",
    "    arr = tf.reshape(arr,list(FLAGS.arr_shape))\n",
    "#     arr = tf.cast(arr,tf.int32)\n",
    "#     label = tf.cast(features['label'],tf.int32)\n",
    "    label = features['label']\n",
    "    \n",
    "    return arr,label\n",
    "\n",
    "\n",
    "def get_iterator(for_training=True,num_epochs=1):\n",
    "    if not num_epochs:\n",
    "        num_epochs = None\n",
    "    root_dir = FLAGS.root_dir\n",
    "    filename = os.path.join(root_dir,('training' if for_training else 'test')+'.tfrecords')\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        dataset = tf.data.TFRecordDataset(filename)\n",
    "#         pdb.set_trace()\n",
    "        dataset = dataset.map(decode)\n",
    "        if for_training:\n",
    "            dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(FLAGS.batch_size)\n",
    "        if for_training:\n",
    "            train_iterator = dataset.make_one_shot_iterator()\n",
    "            val_iterator = dataset.make_one_shot_iterator()\n",
    "            iterators = [train_iterator,val_iterator]\n",
    "        else:\n",
    "            test_iterator = dataset.make_initializable_iterator()\n",
    "            iterators = [test_iterator]\n",
    "\n",
    "        \n",
    "    return iterators\n",
    "\n",
    "    \n",
    "def run_training():\n",
    "    with tf.Graph().as_default():\n",
    "        num_epochs = FLAGS.num_epochs\n",
    "        iterators = get_iterator(num_epochs=num_epochs)\n",
    "        handle = tf.placeholder(tf.string,shape=[])\n",
    "        iterator = tf.data.Iterator.from_string_handle(handle, iterators[0].output_types)\n",
    "        arr_batch,label_batch = iterator.get_next()\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "\n",
    "        X = tf.reshape(arr_batch, [-1]+list(FLAGS.arr_shape)+[1])\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        \n",
    "        is_training_forBN = tf.placeholder(tf.bool, name='is_training_forBN')\n",
    "        \n",
    "        predicted_age,l2_loss = inference(X,keep_prob,is_training_forBN)\n",
    "\n",
    "        loss = get_loss(predicted_age,label_batch,l2_loss)\n",
    "        acc = get_accuracy(predicted_age,label_batch)\n",
    "    \n",
    "        train_op = trainning(loss)\n",
    "    \n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            losses = []\n",
    "            acces = []\n",
    "            steps = []\n",
    "            \n",
    "            test_losses = []\n",
    "            test_acces = []\n",
    "            test_steps = []\n",
    "            \n",
    "#             pdb.set_trace()\n",
    "            sess.run(init_op)\n",
    "            train_iterator_handle = sess.run(iterators[0].string_handle())\n",
    "            val_iterator_handle = sess.run(iterators[1].string_handle())\n",
    "            \n",
    "            test_iterator = get_iterator(for_training=False)[0]\n",
    "            test_iterator_handle = sess.run(test_iterator.string_handle())\n",
    "            \n",
    "            print('Let\\'s get started to train!')\n",
    "            \n",
    "            try:\n",
    "                step = 0\n",
    "                min_test_loss = 300.0\n",
    "                while True:\n",
    "                    start_time = time.time()\n",
    "                    train_a,train_l,_ = sess.run([arr_batch,label_batch,train_op],\n",
    "                                                  feed_dict={keep_prob:0.5,\n",
    "                                                             is_training_forBN:True,\n",
    "                                                            handle:train_iterator_handle})\n",
    "\n",
    "                    val_a,val_l,acc_value,loss_value = sess.run([arr_batch,label_batch,acc,loss],\n",
    "                                                                feed_dict={keep_prob:1.0,\n",
    "                                                                           is_training_forBN:False,\n",
    "                                                                          handle:val_iterator_handle})\n",
    "                    duration = time.time() - start_time\n",
    "\n",
    "                    if step % 100 == 0:\n",
    "                        sess.run(test_iterator.initializer)\n",
    "                        test_predicted_ages = []\n",
    "                        test_labels = []\n",
    "                        try:\n",
    "                            while True:\n",
    "                                test_predicted_age,test_label = sess.run([predicted_age,label_batch],\n",
    "                                                                feed_dict={keep_prob:1.0,\n",
    "                                                                           is_training_forBN:False,\n",
    "                                                                          handle:test_iterator_handle})\n",
    "#                                 pdb.set_trace()\n",
    "                                test_predicted_ages.append(test_predicted_age)\n",
    "                                test_labels.append(test_label)\n",
    "                        except tf.errors.OutOfRangeError:\n",
    "#                             pdb.set_trace()\n",
    "                            test_predicted_ages = np.concatenate(tuple(test_predicted_ages))\n",
    "                            test_labels = np.concatenate(tuple(test_labels))\n",
    "                            test_loss = (get_loss(test_predicted_ages,test_labels,tf.constant(0.0))).eval()\n",
    "                            test_acc = (get_accuracy(test_predicted_ages,test_labels)).eval()\n",
    "                            test_losses.append(test_loss)\n",
    "                            test_acces.append(test_acc)\n",
    "                            test_steps.append(step)\n",
    "                            print('Step %d: training_loss = %.2f (%.3f sec)\\n (val_loss)test_loss = %.2f' \n",
    "                                  %(step,loss_value,duration,test_loss))\n",
    "                            \n",
    "                            if test_loss < min_test_loss:\n",
    "                                min_test_loss = test_loss\n",
    "                                print('best shot model: test_loss = %.2f, step = %d' %(min_test_loss,step))\n",
    "                                if step > 100:\n",
    "                                    save_path = saver.save(sess, FLAGS.saver_dir)\n",
    "                                    print('model saved successfully.')\n",
    "                                    \n",
    "                    steps.append(step)\n",
    "                    acces.append(acc_value)\n",
    "                    losses.append(loss_value)\n",
    "                    step += 1\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print('Done training for %d epochs, %d steps.' %(num_epochs,step))\n",
    "            \n",
    "\n",
    "            f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "            ax1.plot(steps, losses)\n",
    "            ax1.plot(test_steps,test_losses)\n",
    "            ax1.set_title('trainning loss')\n",
    "            ax2.plot(steps, acces)\n",
    "            ax2.plot(test_steps,test_acces)\n",
    "            ax2.set_title('trainning acc')\n",
    "            ax1.grid(True)\n",
    "            ax2.grid(True)\n",
    "    return\n",
    " \n",
    "    \n",
    "def run_test():\n",
    "    with tf.Graph().as_default():\n",
    "        handle = tf.placeholder(tf.string,shape=[])\n",
    "        test_iterator = get_iterator(for_training=False)[0]\n",
    "        iterator = tf.data.Iterator.from_string_handle(handle, test_iterator.output_types)\n",
    "        arr_batch,label_batch = iterator.get_next()\n",
    "        X = tf.reshape(arr_batch, [-1]+list(FLAGS.arr_shape)+[1])\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        \n",
    "        is_training_forBN = tf.placeholder(tf.bool, name='is_training_forBN')\n",
    "        \n",
    "        predicted_age,l2_loss = inference(X,keep_prob,is_training_forBN)\n",
    "\n",
    "        loss = get_loss(predicted_age,label_batch,l2_loss)\n",
    "        acc = get_accuracy(predicted_age,label_batch)\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "#         pdb.set_trace()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, FLAGS.saver_dir)\n",
    "            print('Model loaded successfully.')\n",
    "            test_iterator_handle = sess.run(test_iterator.string_handle())\n",
    "            sess.run(test_iterator.initializer)\n",
    "            test_predicted_ages = []\n",
    "            test_labels = []\n",
    "            try:\n",
    "                while True:\n",
    "                    test_predicted_age,test_label = sess.run([predicted_age, label_batch],\n",
    "                                                    feed_dict={keep_prob:1.0,\n",
    "                                                               is_training_forBN:False,\n",
    "                                                              handle:test_iterator_handle})\n",
    "                    test_predicted_ages.append(test_predicted_age)\n",
    "                    test_labels.append(test_label)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                test_predicted_ages = np.concatenate(tuple(test_predicted_ages))\n",
    "                test_labels = np.concatenate(tuple(test_labels))\n",
    "                \n",
    "                test_loss = (get_loss(test_predicted_ages,test_labels,tf.constant(0.0))).eval()\n",
    "                test_acc = (get_accuracy(test_predicted_ages,test_labels)).eval()\n",
    "                \n",
    "                print('test_loss = %.2f, test_accuracy = %.2f.' \n",
    "                                  %(test_loss,test_acc))\n",
    "            plt.title('Test Data')\n",
    "            plt.xlabel('Chronological Age')\n",
    "            plt.ylabel('Brain Age (Predicted)')\n",
    "            plt.xlim(0, 100)\n",
    "            plt.ylim(0, 100)\n",
    "            for i in range(len(test_labels)):\n",
    "                plt.scatter(test_labels[i], test_predicted_ages[i], c = 'blue',s=1)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "    return\n",
    "\n",
    "\n",
    "def main(_):\n",
    "#     pdb.set_trace()\n",
    "    arr = np.load('./IXI_npy/mean_npy.npy')\n",
    "    FLAGS.arr_shape = arr.shape\n",
    "    \n",
    "    run_training()\n",
    "    run_test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--saver_dir',\n",
    "                       type=str,\n",
    "                       default=\"./log/demo1.1_mine_model.ckpt\",\n",
    "                       help='Directory to save checkpoint.')\n",
    "    parser.add_argument('--l2_epsilon',\n",
    "                       type=float,\n",
    "                       default=1e-6,\n",
    "                       help='L2 regularization parameter.')\n",
    "    parser.add_argument('--root_dir',\n",
    "                       type=str,\n",
    "                       default='./',\n",
    "                       help='Where is the tfrecord file.')\n",
    "    parser.add_argument('--sizeof_kernel_1',\n",
    "                        type=int,\n",
    "                        default=16,\n",
    "                        help='kernel size of 1st hidden layer.')\n",
    "    parser.add_argument('--batch_size',\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                        help='Batch size.')\n",
    "#     parser.add_argument(\n",
    "#       '--learning_rate',\n",
    "#       type=float,\n",
    "#       default=0.01,\n",
    "#       help='Initial learning rate.')\n",
    "    parser.add_argument('--num_epochs',\n",
    "                        type=int,\n",
    "                        default=50,\n",
    "                        help='Number of epochs to run trainer.')\n",
    "#     parser.add_argument(\n",
    "#       '--hidden1',\n",
    "#       type=int,\n",
    "#       default=128,\n",
    "#       help='Number of units in hidden layer 1.')\n",
    "#     parser.add_argument(\n",
    "#       '--hidden2',\n",
    "#       type=int,\n",
    "#       default=32,\n",
    "#       help='Number of units in hidden layer 2.')\n",
    "\n",
    "#     parser.add_argument(\n",
    "#       '--train_dir',\n",
    "#       type=str,\n",
    "#       default='/tmp/data',\n",
    "#       help='Directory with the training data.')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_img = np.load('./IXI_npy/origin/IXI012.npy')\n",
    "print(npy_img.shape)\n",
    "plt.imshow(npy_img[:,:,60])\n",
    "# plt.imshow(npy_img[:,60,:])\n",
    "# plt.imshow(npy_img[60,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(10,size=(3,3))\n",
    "b = np.random.randint(10,size=(3,3))\n",
    "c = np.random.randn(3,3)\n",
    "print(c)\n",
    "# a += 1\n",
    "# print(a)\n",
    "cc = c.astype(int)\n",
    "print(cc.dtype)\n",
    "# print(c)\n",
    "# print(c)\n",
    "# c.astype(int)\n",
    "# preprocess_1('IXI002-Guys-0828-T1.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotypics = pd.read_csv('./phenotypics.csv', sep=',',header=0)\n",
    "# print(phenotypics)\n",
    "phenotypics[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /media/woody/Elements/age_data/IXI/IXI-T1/\n",
    "\n",
    "os.path.exists('/media/woody/Elements/age_data/IXI/IXI-T1/IXI.*-.*-.*-T1\\.nii\\.gz$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from progressbar import *\n",
    "\n",
    "# for i in range(10):\n",
    "#     print('\\r'+str((i+1)*100.0/10)+'%',end='',flush=True)\n",
    "#     time.sleep(0.3)\n",
    "# print('abc')\n",
    "\n",
    "pbar = ProgressBar().start()\n",
    "for i in range(10):\n",
    "    pbar.update(int(i*100/(10-1)))\n",
    "    time.sleep(0.8)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from progressbar import *\n",
    "total = 100\n",
    "def dosomework():\n",
    "    time.sleep(0.01)\n",
    "widgets = ['Progress: ',Percentage(), ' ', Bar('>'),' ', Timer(),\n",
    "      ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets, maxval=total+100).start()\n",
    "for i in range(total):\n",
    "    # do something\n",
    "    pbar.update(i)\n",
    "    dosomework()\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProgressBar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import sys, time  \n",
    "from progressbar import *  \n",
    "  \n",
    "total = 1000  \n",
    "  \n",
    "def dosomework():  \n",
    "    time.sleep(0.01)  \n",
    "pbar = ProgressBar().start()  \n",
    "for i in range(1000):  \n",
    "    pbar.update(int((i / (total - 1)) * 100))  \n",
    "    dosomework()  \n",
    "pbar.finish()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xlrd.open_workbook('IXI.xls','rb')\n",
    "print('工作表名为：'+ data.sheet_names()[0])\n",
    "table = data.sheets()[0]\n",
    "nrows = table.nrows\n",
    "ncols = table.ncols\n",
    "print('表格行数为%d,列数为%d'%(nrows,ncols))\n",
    "\n",
    "#输出每一行的值\n",
    "# for item in range(table.nrows):\n",
    "#     print(table.row_values(item))\n",
    "\n",
    "#获取单元格的值\n",
    "cell_A1 = table.row(0)[0].value\n",
    "cell_A2 = table.cell(0,0).value\n",
    "cell_A3 = table.col(0)[0].value\n",
    "\n",
    "print(cell_A1)\n",
    "print(cell_A2)\n",
    "print(cell_A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
